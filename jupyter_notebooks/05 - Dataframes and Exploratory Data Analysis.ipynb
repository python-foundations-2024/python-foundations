{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Key Statistical Concepts"
   ],
   "metadata": {
    "id": "B5bR_185vAjd"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "\n",
    "### Mean\n",
    "The mean is the average of a dataset and is calculated by summing all the values and dividing by the total number of observations. It provides a central value of the data.\n",
    "\n",
    "- Example:\n",
    "\n",
    "In a data analytics scenario, suppose we have the following sales data for a week (in thousands of dollars): [10, 15, 20, 25, 30]. The mean or average will be 20\n",
    "\n",
    "```python\n",
    "sales = [10, 15, 20, 25, 30]\n",
    "mean_sales = sum(sales) / len(sales)\n",
    "print(f\"Mean Sales:{mean_sales}\") # Output: Mean Sales: 20.0\n",
    "```\n"
   ],
   "metadata": {
    "id": "eLhAAKaGu9Us"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# We can use numpy to calculate it\n",
    "import numpy as np\n",
    "\n",
    "# Sales data\n",
    "sales = [10, 15, 20, 25, 30]\n",
    "\n",
    "print(f\"Mean:{np.mean(sales)}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ep75_0sEvE6y",
    "outputId": "cbb39ab1-db75-435e-86be-a1fff33e2225"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Median\n",
    "The median is the middle value of a dataset when sorted in ascending order. It is useful in datasets that may contain outliers.\n",
    "\n",
    "- Example:\n",
    "\n",
    "Consider customer ages: [22,25,29,50,35], the median age, the one in the middle is 29\n",
    "\n",
    "```python\n",
    "ages = [22, 25, 29, 50, 35]\n",
    "sorted_ages = sorted(ages)\n",
    "n = len(sorted_ages)\n",
    "median_age = sorted_ages[n // 2] if n % 2 != 0 else (sorted_ages[n // 2 - 1] + sorted_ages[n // 2]) / 2\n",
    "print(f\"Median Age: {median_age}\") # Output: Median Age: 29\n",
    "```\n"
   ],
   "metadata": {
    "id": "0cXAwEsgwKZd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#using NumPy\n",
    "import numpy as np\n",
    "ages = [22, 25, 29, 50, 35]\n",
    "print(f\"Median Age:{np.median(ages)}\"): # Output: Median Age: 29"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_fQFnHk9vnmh",
    "outputId": "bfde83c9-95a2-4021-f136-98da72e0ae4d"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Mode\n",
    "The mode is the value that appears most frequently in a dataset. It is particularly useful for categorical data.\n",
    "\n",
    "Example:\n",
    "In a survey about favorite fruits, the responses are:\n",
    "[\"Apple\",\"Banana\",\"Apple\",\"Orange\",\"Banana\",\"Banana\"]\n",
    "The mode, or the most frequent value is \"Banana\"\n",
    "\n",
    "```python\n",
    "from statistics import mode\n",
    "\n",
    "fruits = [\"Apple\", \"Banana\", \"Apple\", \"Orange\", \"Banana\", \"Banana\"]\n",
    "mode_fruit = mode(fruits)\n",
    "print(f\"Mode of Fruits:{mode_fruit}\") # Output: Mode of Fruits: Banana\n",
    "```\n",
    "> For numerical data, NumPy also provides a more direct way to calculate the mode using `scipy.stats.mode()`"
   ],
   "metadata": {
    "id": "kiK1ETk8wqge"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from statistics import mode\n",
    "\n",
    "fruits = [\"Apple\", \"Banana\", \"Apple\", \"Orange\", \"Banana\", \"Banana\"]\n",
    "mode_fruit = mode(fruits)\n",
    "print(f\"Mode of Fruits:{mode_fruit}\") # Output: Mode of Fruits: Banana"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NMDlLh5bwrUa",
    "outputId": "3473dec2-71fc-4909-adb5-3eb966076d35"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Variance\n",
    "Variance measures how far each number in the dataset is from the mean and indicates the degree of spread in the data.\n",
    "\n",
    "- Example:\n",
    "\n",
    "In analyzing test scores: [80,85,90,95,100]\n",
    "\n",
    "Mean =\n",
    "$\\frac{80 + 85 + 90 + 95 + 100}{5}$ = $\\frac{450}{5}$ = 90\n",
    "\n",
    "Calculate the Squared Differences from the Mean:\n",
    "\n",
    "- $(80-90)^2 = 100$\n",
    "- $(85-90)^2 = 25$\n",
    "- $(90-90)^2 = 0$\n",
    "- $(95-90)^2 = 25$\n",
    "- $(100-90)^2 = 100$\n",
    "\n",
    "Sum the Squared Differences:\n",
    "100+25+0+25+100=250\n",
    "\n",
    "Calculate the Variance (using sample variance, which divides by $n−1$):\n",
    "\n",
    "Variance = $\\frac{250}{5 - 1}$ = $\\frac{250}{4}$ = 62.5\n",
    "\n",
    "\n",
    "```python\n",
    "scores = [80, 85, 90, 95, 100]\n",
    "mean_score = sum(scores) / len(scores)\n",
    "variance = sum((x - mean_score) ** 2 for x in scores) / (len(scores) - 1)\n",
    "print(f\"Variance of Scores: {variance}\") # Output: Variance of Scores: 62.5\n",
    "```\n"
   ],
   "metadata": {
    "id": "BL-aF-jMx2Jv"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Using NumPy\n",
    "import numpy as np\n",
    "\n",
    "# List of scores\n",
    "scores = [80, 85, 90, 95, 100]\n",
    "\n",
    "# Calculate mean using NumPy\n",
    "mean_score_np = np.mean(scores)\n",
    "\n",
    "# Calculate variance using NumPy (ddof=1 for sample variance)\n",
    "variance_np = np.var(scores, ddof=1)\n",
    "\n",
    "print(f\"Variance of Scores (NumPy): {variance_np}\") # Output: Variance of Scores (NumPy): 62.5"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O1WPGrCdyJFC",
    "outputId": "995f3afe-e272-4fcb-bc5b-669316925188"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "> The degrees of freedom (ddof) parameter is key to understanding how variance is calculated. When you're calculating variance, you have two options: **population variance** (ddof=0) or **sample variance** (ddof = 1). When you're working with a sample, you typically use ddof=1 to ensure an unbiased estimate of the population variance. When you have data for the entire population, you can use ddof=0."
   ],
   "metadata": {
    "id": "1Q7QimK2ypCW"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Standard Deviation\n",
    "The standard deviation is the square root of the variance and provides a measure of the average distance of each data point from the mean.\n",
    "\n",
    "- Example:\n",
    "\n",
    "Using the same test scores:\n",
    "[80,85,90,95,100] the standard deviation is $\\sqrt{62.5}$ = 7.9\n",
    "\n",
    "```python\n",
    "import math\n",
    "\n",
    "std_dev = math.sqrt(variance)\n",
    "print(f\"Standard Deviation of Scores: {std_dev}\") # Output: Standard Deviation of Scores: 7.905694150420948\n",
    "```\n"
   ],
   "metadata": {
    "id": "uHIATJk8ziwq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Using NumPy\n",
    "import numpy as np\n",
    "\n",
    "# List of scores\n",
    "scores = [80, 85, 90, 95, 100]\n",
    "\n",
    "# Sample standard deviation (ddof=1)\n",
    "sample_std_dev = np.std(scores, ddof=1)  # Sample standard deviation\n",
    "\n",
    "print(f\"Sample Standard Deviation: {sample_std_dev}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l6XtthxvznUp",
    "outputId": "b5e754bc-90b2-45de-e3c3-08b200b17023"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Quantiles\n",
    "Quantiles divide a dataset into equal-sized subsets. For instance, quartiles divide the data into four equal parts.\n",
    "Quantiles are useful to identify outliers in the data\n",
    "\n",
    "- Example:\n",
    "\n",
    "Consider the following exam scores:\n",
    "[60,70,80,90,100]\n",
    "\n",
    "```python\n",
    "scores = [60, 70, 80, 90, 100]\n",
    "sorted_scores = sorted(scores)\n",
    "\n",
    "# Calculate quartiles\n",
    "q1 = sorted_scores[int(len(sorted_scores) * 0.25)]\n",
    "q2 = sorted_scores[int(len(sorted_scores) * 0.5)]  # Median\n",
    "q3 = sorted_scores[int(len(sorted_scores) * 0.75)]\n",
    "\n",
    "print(f\"1st Quartile (Q1):{q1}\") # Output: 1st Quartile (Q1):70\n",
    "print(f\"Median (Q2):{q2}\") # Output: Median (Q2):80\n",
    "print(f\"3rd Quartile (Q3):{q3}\") # Output: 3rd Quartile (Q3):90\n",
    "```"
   ],
   "metadata": {
    "id": "rQcrFW8zRBo_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Using NumPy\n",
    "import numpy as np\n",
    "\n",
    "# Scores\n",
    "scores = [60, 70, 80, 90, 100]\n",
    "\n",
    "# Calculate the quartiles using NumPy's percentile function\n",
    "q1 = np.percentile(scores, 25)  # 1st Quartile (Q1) - 25th percentile\n",
    "q2 = np.percentile(scores, 50)  # 2nd Quartile (Q2 - Median) - 50th percentile\n",
    "q3 = np.percentile(scores, 75)  # 3rd Quartile (Q3) - 75th percentile\n",
    "\n",
    "# Print the results\n",
    "print(f\"1st Quartile (Q1): {q1}\")\n",
    "print(f\"Median (Q2): {q2}\")\n",
    "print(f\"3rd Quartile (Q3): {q3}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EgSL5KivSSts",
    "outputId": "ebfe9d74-270c-458a-cde1-3459d5c94263"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Dataframes\n",
    "\n",
    "In Python, dataframes are widely used data structures that represent data in a tabular form—organized in rows and columns, much like a spreadsheet or a SQL table. Dataframes make data analysis, manipulation, and visualization much easier, especially when working with large datasets.\n",
    "\n",
    "**What is a DataFrame?**\n",
    "\n",
    "A DataFrame is essentially a two-dimensional, labeled data structure with columns of potentially different types. This versatility allows users to work with heterogeneous data types within a single structure, making dataframes very powerful for data science and machine learning.\n",
    "\n",
    "**Key characteristics of a DataFrame:**\n",
    "\n",
    "- Two-dimensional: It has rows and columns, which gives it a tabular structure.\n",
    "- Labeled axes: Rows and columns can be labeled, making it easy to reference parts of the DataFrame by name rather than by position.\n",
    "- Flexible data types: Each column can hold a different data type (e.g., integers, floats, strings, dates).\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "9oO9Dp_gV0n2"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Types of DataFrames in Python\n",
    "While pandas is the most widely used library for dataframes in Python, other libraries implement dataframes with additional functionality or optimizations tailored for specific use cases.\n",
    "\n",
    "Here are some of the primary types of dataframes in Python:\n",
    "\n",
    "| **DataFrame Type** | **Best Use Case**                                                  | **Memory Usage**                           | **Description**                                                                                 |\n",
    "|--------------------|--------------------------------------------------------------------|--------------------------------------------|-------------------------------------------------------------------------------------------------|\n",
    "| **[Pandas](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html)**         | Small to medium datasets that fit in memory                       | In-memory                                  | Standard Python library for data manipulation and analysis.                                     |\n",
    "| **[Dask](https://docs.dask.org/en/stable/dataframe.html)**           | Large datasets that exceed memory limits, single-machine parallel | Out-of-core (chunk-based, parallel)        | Extension of pandas for handling large data, processing in parallel.                            |\n",
    "| **[Koalas/PySpark](https://koalas.readthedocs.io/en/latest/reference/frame.html#constructor)** | Big data processing on distributed cluster with Spark             | Distributed (cluster-based)                | Spark-compatible dataframe for big data, scalable to clusters.                                  |\n",
    "| **[Modin](https://modin.readthedocs.io/en/latest/usage_guide/index.html)**          | Pandas-compatible workflows with faster, parallel execution       | In-memory (parallel)                       | Parallelized pandas replacement for faster execution.                                           |\n",
    "| **[Polars](https://docs.pola.rs/)**         | Performance-critical applications on large datasets               | In-memory (optimized for speed)            | Rust-based dataframe optimized for speed, available in Python.                                  |\n",
    "| **[Vaex](https://vaex.io/docs/api.html)**           | Extremely large datasets without loading all data into memory     | Out-of-core (efficient memory usage)       | Handles large datasets out-of-core; efficient for exploration and stats.                        |\n"
   ],
   "metadata": {
    "id": "zkrPnky5W7CV"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "<img src=\"https://pandas.pydata.org/static/img/pandas_white.svg\" alt=\"Pandas Logo\" height=\"50\" >\n",
    "\n",
    "<img src=\"https://docs.dask.org/en/stable/_static/images/dask-horizontal-white.svg\" alt=\"Dask Logo\" height=\"50\" >\n",
    "\n",
    "<img src=\"https://koalas.readthedocs.io/en/latest/_static/koalas-logo-docs.png\" alt=\"Koalas Logo\" height=\"50\" >\n",
    "\n",
    "<img src=\"https://modin.readthedocs.io/en/latest/_images/MODIN_ver2_hrz.png\" alt=\"Modin Logo\" height=\"50\" >\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/pola-rs/polars-static/master/logos/polars-logo-dimmed-medium.png\" alt=\"Polars Logo\" height=\"50\" >\n",
    "\n",
    "<img src=\"https://vaex.io/docs/_static/logo-grey.svg\" alt=\"Vaex Logo\" height=\"50\" >\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "zdSkQtWIZCTl"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pandas\n",
    "\n",
    "Pandas is a powerful and widely-used library in Python for data manipulation and analysis. It provides flexible and efficient data structures, primarily\n",
    "**Series** (1-dimensional) and **DataFrame** (2-dimensional), that make it easy to work with structured data. The DataFrame is especially valuable for data analytics as it allows you to store, filter, transform, and analyze datasets much like a spreadsheet or SQL table.\n",
    "\n",
    "- Data Loading and Cleaning: Pandas can load data from various sources (CSV, Excel, SQL databases, etc.) and provides tools for handling missing values, duplicates, and other inconsistencies.\n",
    "- Data Transformation: You can easily filter rows, select columns, group data, and apply complex transformations.\n",
    "- Data Aggregation and Summary Statistics: Pandas provides efficient ways to compute summary statistics (mean, sum, median, etc.) for each group or column.\n",
    "- Data Visualization Integration: Pandas integrates well with libraries like Matplotlib and Seaborn for quick visualizations.\n",
    "\n",
    "Let’s go through some examples that demonstrate common tasks in data analytics using Pandas.\n",
    "\n",
    "> For more details on especific functions [read the docs](https://pandas.pydata.org/docs/reference/index.html)\n"
   ],
   "metadata": {
    "id": "p7ojhl5Hcu80"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Installing pandas\n",
    "\n",
    "Installing pandas depends on the environment you're using.\n",
    "\n",
    "1. In Jupyter notebooks, if you need to update pandas to a specific version, you can do the following:\n",
    "\n",
    "  ```python\n",
    "  # Install or update pandas to last version\n",
    "  !pip install pandas --upgrade\n",
    "  ```\n",
    "  Run this in a Colab cell, and it will install or update pandas to the latest version.\n",
    "\n",
    "  You can install a especific version of pandas using the `==` syntax.\n",
    "  ```python\n",
    "  # Install a certain version of pandas\n",
    "  !pip install pandas==1.5.3\n",
    "```\n",
    "> **Google Colab** comes with pandas pre-installed, so you typically don't need to install it manually.\n",
    "\n",
    "2. Installing Pandas in Jupyter Notebook or Local Python Environment\n",
    "If you’re working locally in a Jupyter Notebook or any Python environment (like PyCharm, VS Code, etc.), you can use `pip to install pandas`.\n",
    "\n",
    "  Open a terminal and run:\n",
    "\n",
    "  ```bash\n",
    "  pip install pandas\n",
    "  ```\n",
    "  If you want a specific version of pandas, specify it as follows:\n",
    "\n",
    "  ```bash\n",
    "  pip install pandas==1.5.3\n",
    "  ```\n",
    "\n",
    "3. Installing Pandas in Anaconda\n",
    "If you’re using Anaconda, it’s often best to install pandas via the `conda` package manager, which manages dependencies more effectively within the Anaconda ecosystem.\n",
    "\n",
    "  Using Conda\n",
    "  Open the Anaconda Prompt and enter:\n",
    "\n",
    "  ```bash\n",
    "  conda install pandas\n",
    "  ```\n",
    "  This will install pandas and any required dependencies.\n",
    "\n",
    "  If you want a specific version of pandas, you can specify it like this:\n",
    "\n",
    "  ```bash\n",
    "  conda install pandas=1.5.3\n",
    "  ```\n",
    "  Creating a New Environment with Pandas\n",
    "  You can also create a new conda environment with pandas pre-installed:\n",
    "\n",
    "  ```bash\n",
    "  conda create -n myenv pandas\n",
    "  ```\n",
    "  Replace `myenv` with your desired environment name. To activate the environment, use:\n",
    "\n",
    "  ```bash\n",
    "  conda activate myenv\n",
    "  ```\n",
    "\n",
    "4. Verifying the Installation\n",
    "To check that pandas installed correctly, open a Python environment (Colab, Jupyter, or terminal) and run:\n",
    "\n",
    "  ```python\n",
    "  import pandas as pd\n",
    "  print(pd.__version__)  # This should display the installed pandas version\n",
    "  ```\n",
    "  This will confirm that pandas is installed and display the version."
   ],
   "metadata": {
    "id": "govF6OAjf8dD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "print(pd.__version__)  # This should display the installed pandas version"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ea9RgLgPiZyn",
    "outputId": "18bcf83b-039d-4880-bb40-1e4eed1078d5"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using Interactivity in Google Colab\n",
    "\n",
    "Interactive tables enable you to visualize, filter, and manipulate data in real time, making it easier to derive insights and present findings effectively.\n",
    "\n",
    "1. Interactive tables\n",
    "```python\n",
    "from google.colab import data_table\n",
    "data_table.enable_dataframe_formatter()\n",
    "```\n",
    "\n",
    "2. Interactive sheets\n",
    "```python\n",
    "# This will create a new interactive google sheet with the data in the dataframe passed as paramter (results_df)\n",
    "from google.colab import sheets\n",
    "sheet = sheets.InteractiveSheet(df=results_df)\n",
    "```"
   ],
   "metadata": {
    "id": "ND0eeADq2heX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import data_table\n",
    "data_table.enable_dataframe_formatter()\n"
   ],
   "metadata": {
    "id": "OuV76hzb2sGE"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can test it with any of the sample datasets provided:"
   ],
   "metadata": {
    "id": "0jsw06NM_a2L"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from vega_datasets import data\n",
    "print(data.list_datasets())\n",
    "cars = data.cars()\n",
    "cars"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1521
    },
    "id": "lFMaqTIz_bPH",
    "outputId": "327a88d7-947f-46aa-cd34-8b52c36cedef"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Working with files\n",
    "When working with Pandas, the preferred file formats depend on your specific use case, data size, and performance requirements.\n",
    "\n",
    "The most common are:\n",
    "\n",
    "| **File Format**  | **Best for**                                  | **Pros**                                                       | **Cons**                                  |\n",
    "|------------------|-----------------------------------------------|---------------------------------------------------------------|-------------------------------------------|\n",
    "| **CSV**          | Small to medium datasets, interoperability   | Simple, human-readable, supported everywhere                  | Slower for large datasets, lacks metadata |\n",
    "| **Parquet**      | Large datasets, big data, columnar storage   | Fast, compressed, efficient for large datasets, schema support | Not human-readable                       |\n",
    "| **Feather**      | Fast I/O, sharing data between Python & R     | Fast, optimized for in-memory operations, cross-language support | Not as compressed as Parquet             |\n",
    "| **HDF5**         | Large datasets, hierarchical data            | Efficient for large datasets, supports compression and chunking | Not human-readable, requires extra setup |\n",
    "| **Excel (XLSX)** | Business environments, small to medium datasets | Widely used, easy to open/edit, supports multiple sheets        | Slower, larger file size, less efficient |\n",
    "| **SQL Databases**| Persistent storage, large datasets with querying | Efficient querying, persistent, scalable storage               | Requires setup and maintenance           |\n",
    "| **JSON**         | Data exchange between systems, hierarchical data | Flexible, human-readable, supports nested data structures      | Can be verbose, less efficient for large datasets |\n",
    "\n",
    "\n",
    "> More details in https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html\n",
    "\n",
    "\n",
    "We are going to work mostly with CSV files in the following examples for simplicity."
   ],
   "metadata": {
    "id": "nsbbtRLa3Wjd"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Read from file\n",
    "The `pandas.read_csv()` function allows you to read CSV files into a DataFrame, which is the core data structure in Pandas. The basic syntax is:\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Read a CSV file into a DataFrame\n",
    "df = pd.read_csv('path_to_file.csv')\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n",
    "```\n",
    "Parameters of read_csv() (key ones):\n",
    "- filepath_or_buffer: The path to the CSV file you want to read (can be a URL, local file path, or file-like object).\n",
    "- sep: The delimiter that separates columns (default is comma ,). You can change it for other delimiters like - tabs (\\t), semicolons (;), etc.\n",
    "- header: Row number to use as column names (default is 0, meaning the first row is used for column names).\n",
    "- index_col: Column(s) to set as the index (default is None).\n",
    "- usecols: Specify a subset of columns to read (can be a list of column names or indices).\n",
    "- dtype: Specify the data type for one or more columns.\n",
    "- na_values: Additional strings to recognize as NA/NaN.\n",
    "- skiprows: Number of lines to skip from the top of the file.\n",
    "\n",
    "> For more details you can read the docs: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html"
   ],
   "metadata": {
    "id": "2OhTS1QZ1IOw"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Write to file\n",
    "Once you have a DataFrame, you can write it to a CSV file using the `to_csv()` function. The basic syntax is:\n",
    "```python\n",
    "df.to_csv('output_file.csv', index=False)\n",
    "```\n",
    "Parameters of to_csv() (key ones):\n",
    "- path_or_buffer: The file path or buffer where the CSV will be saved.\n",
    "- sep: The delimiter to use (default is comma ,).\n",
    "- index: Whether to write row names (index) (default is True). If you don't want to include the index in the output file, set index=False.\n",
    "- header: Whether to write the column names (default is True).\n",
    "- columns: A subset of columns to write.\n",
    "- mode: File opening mode ('w' for write, 'a' for append).\n",
    "- na_rep: String to replace missing values (default is '').\n",
    "- encoding: The encoding format to use for saving the file (e.g., 'utf-8').\n",
    "\n",
    "> For more details read the docs https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_csv.html"
   ],
   "metadata": {
    "id": "z5nufhsS2Jx5"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exploratory Data Analysis with Pandas"
   ],
   "metadata": {
    "id": "EaID98NM07PI"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Importing the Library and Loading Data\n",
    "First, let's import pandas and load a sample dataset. Here, we’ll use a sample CSV file, which could represent any real-world dataset.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Load data from a CSV file\n",
    "# In real cases, you would use a file path, e.g., 'data.csv'\n",
    "df = pd.read_csv(\"sample_data.csv\")  # Replace with the path to your CSV file\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "df.head()\n",
    "```\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "2poiuogHdQOo"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can use the following to upload files to your Google Colab environment.\n",
    "Please, note that the files will be removed when the session is finished, so keep a local copy."
   ],
   "metadata": {
    "id": "EcpxRPPp5gCt"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "uMzgjs2WfbBm",
    "outputId": "ccd00608-dc4a-4ecb-8e1f-6dd4b49c43a0"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data from a CSV file\n",
    "df = pd.read_csv(\"sample_data/california_housing_test.csv\")\n"
   ],
   "metadata": {
    "id": "lCSjhwITifOh"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "##  Understading the Data"
   ],
   "metadata": {
    "id": "E6rd1bN96WOF"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "### Structure of the DataFrame\n",
    "\n",
    "To get a quick overview of the DataFrame, use the **info** command:\n",
    "\n",
    "```python\n",
    "df.info()\n",
    "```\n",
    "\n",
    "The **shape** command will tell us the number of rows and columns of the data\n",
    "```python\n",
    "df.shape\n",
    "```\n",
    "With the **head** command we can get the first X rows of the dataframe\n",
    "```python\n",
    "df.head(5)\n",
    "```\n",
    "If there are multiple columns in the dataframe, by default it will appear with ... hiding some of the columns in the middle. Exploring the data we are interested in viewing all the columns, to do that we can use the set_option of pandas and indicate a number of columns that is big enough to accomodate all the columns that we have.\n",
    "\n",
    "```python\n",
    "pd.set_option('max_columns', 200)\n",
    "df.head(5)\n",
    "```\n",
    "\n",
    "We can list the column names using the **columns** command\n",
    "```python\n",
    "df.columns\n",
    "```\n",
    "\n",
    "Each column in a dataframe is a series of data, and as such it´s associated with a data type in pandas.\n",
    "To list the data types for each column we use the dtypes command\n",
    "```python\n",
    "df.dtypes\n",
    "```\n",
    "\n",
    "\n",
    "**Describe** will show you the number of entries, column names, non-null counts, and data types.\n",
    "\n",
    "```python\n",
    "df.describe()\n",
    "```\n",
    "The output is another DataFrame that includes several key statistics for each numerical column:\n",
    "\n",
    "- count: The number of non-null entries.\n",
    "- mean: The average value.\n",
    "- std: The standard deviation, which measures the amount of variation or dispersion in the data.\n",
    "- min: The minimum value.\n",
    "- 25%: The first quartile, or the 25th percentile.\n",
    "- 50%: The median, or the 50th percentile.\n",
    "- 75%: The third quartile, or the 75th percentile.\n",
    "- max: The maximum value.\n",
    "\n",
    "| **Aspect**            | **`df.info()`**                                          | **`df.describe()`**                                      |\n",
    "|-----------------------|----------------------------------------------------------|----------------------------------------------------------|\n",
    "| **Purpose**           | Overview of DataFrame structure and missing data         | Summary statistics of numerical columns                  |\n",
    "| **Output**            | Data types, non-null counts, memory usage                | Statistical summary (mean, min, max, std, etc.)          |\n",
    "| **Works on**          | All columns, including non-numeric columns               | By default, only numerical columns                       |\n",
    "| **Missing Values**    | Shows count of non-null values                          | Shows count of non-null values (for each column)         |\n",
    "| **Data Types**        | Shows data types (e.g., `int64`, `float64`)              | Doesn't show data types, only summary stats              |\n",
    "| **Statistical Summary**| Does not provide any statistical measures               | Provides mean, std, min, max, percentiles, etc.          |\n",
    "| **Categorical Data**  | Shows dtype for object columns (strings)                 | Can summarize categorical data if specified (`include='object'`) |\n",
    "| **When to Use**       | Use when you need a quick check on the **structure** of the DataFrame, including data types, non-null counts, and memory usage | Use when you want to quickly get **summary statistics** for the numerical columns, such as mean, median, standard deviation, and percentiles |"
   ],
   "metadata": {
    "id": "JC9WFHes7CPw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i9gTCU6K4JLU",
    "outputId": "fab907a8-90ce-4141-863a-c3d110e0d490"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Display the first few rows of the dataset\n",
    "df.head()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "VLg6dlc44Hm8",
    "outputId": "e4f0416f-94c6-48e5-e446-37ded739b602"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df.columns"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pCFndmv95FYV",
    "outputId": "cb7c0707-f593-4462-90e5-d10adb4bf582"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df.dtypes"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "id": "u9pvpp3D5HeX",
    "outputId": "f325b110-88a8-44d7-89c4-4dc713a99549"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df.info()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i_jGhSUU8T-n",
    "outputId": "b90bad03-8aa8-4a84-a0c0-a8c1fb4e82c7"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df.describe()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "3Dj5G4Fi8Wwd",
    "outputId": "4e383dbb-a7d7-4e79-be9b-1e5b83d9c506"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Listing Columns\n",
    "\n",
    "To list all the columns in the DataFrame:\n",
    "\n",
    "```python\n",
    "df.columns\n",
    "```\n",
    "This is useful if we need to iterate through the columns of the dataframe\n",
    "\n",
    "We can also get the data type of each column with\n",
    "```python\n",
    "df.dtypes\n",
    "```"
   ],
   "metadata": {
    "id": "lK9M1a757NVM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df.columns"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SMRDnjrs8aVo",
    "outputId": "a705f4f3-891b-4f4f-cb8c-1c3a70b09d03"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df.dtypes"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "id": "edAjE9a-8ccV",
    "outputId": "2c1b0f6f-50c0-49e3-b74e-b650a88bbc26"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Finding Minimum and Maximum Values\n",
    "\n",
    "To find the minimum and maximum values of numerical columns you can use the dataframe functions or NumPy as we´ve seen before.\n",
    "\n",
    "```python\n",
    "min_values = df[column].min()\n",
    "max_values = df[column].max()\n",
    "```\n",
    "> You can calculate other values like standard deviation, quantile, mean, mode and more using the same syntax.\n"
   ],
   "metadata": {
    "id": "R3lzLShC7WPu"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Create a dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Calculate metrics for numeric columns\n",
    "numeric_df = df.select_dtypes(include='number')\n",
    "for column in numeric_df.columns:\n",
    "    results[column] = {\n",
    "        'min': numeric_df[column].min(),\n",
    "        'max': numeric_df[column].max(),\n",
    "        'mean': numeric_df[column].mean(),\n",
    "        'std': numeric_df[column].std(),\n",
    "        'count': numeric_df[column].count(),\n",
    "        'p90': numeric_df[column].quantile(0.25),\n",
    "        'p75': numeric_df[column].quantile(0.75),\n",
    "        'p50': numeric_df[column].quantile(0.5),\n",
    "        'p25': numeric_df[column].quantile(0.25),\n",
    "        'unique_count': numeric_df[column].nunique(),\n",
    "        'most_common': numeric_df[column].mode()[0] if not numeric_df[column].mode().empty else None,\n",
    "        'count_non_null': numeric_df[column].count(),\n",
    "    }\n",
    "\n",
    "# Calculate metrics for non-numeric columns\n",
    "non_numeric_df = df.select_dtypes(exclude='number')\n",
    "for column in non_numeric_df.columns:\n",
    "    results[column] = {\n",
    "        'unique_count': non_numeric_df[column].nunique(),\n",
    "        'most_common': non_numeric_df[column].mode()[0],\n",
    "        'count_non_null': non_numeric_df[column].count(),\n",
    "    }\n",
    "\n",
    "\n",
    "# Convert results to a DataFrame for better visualization\n",
    "# the .T attribute is used to transpose the DataFrame\n",
    "results_df = pd.DataFrame(results).T\n",
    "\n",
    "results_df\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "id": "3DCeGA1ND0po",
    "outputId": "96f4c713-2faa-4cd0-99f6-da12cf0176d3"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Counting Unique Values\n",
    "\n",
    "To get the number of unique values in each column:\n",
    "\n",
    "```python\n",
    "unique_counts = df[column].nunique()\n",
    "print(\"Number of unique values:\\n\", unique_counts)\n",
    "```\n",
    "You can also get the list of unique values using\n",
    "unique_values = df[column].unique()"
   ],
   "metadata": {
    "id": "2-tqAxBw7kIJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Get number of unique values of each column as a dictionary\n",
    "nunique_values_dict = {col: df[col].nunique() for col in df.columns}\n",
    "\n",
    "print(nunique_values_dict)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fc1YuCqw-NfQ",
    "outputId": "87e78ee9-ff32-43eb-8696-89e5b005bc21"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Counting Null Values\n",
    "\n",
    "To count the number of null values in each column:\n",
    "\n",
    "```python\n",
    "null_counts = df.isnull().sum()\n",
    "print(\"Number of null values:\\n\", null_counts)\n",
    "```\n",
    "\n",
    ">`isnull()` and `isna()` are equivalent function to identify the missing values"
   ],
   "metadata": {
    "id": "Dpan7TCmCGCW"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "null_counts = df.isnull().sum()\n",
    "print(\"Number of null values:\\n\", null_counts)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mx-jfWVS-wkX",
    "outputId": "8aeec53c-4907-4b24-e657-fc326ca517de"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Playground to explore with functions to understand the dataframe\n"
   ],
   "metadata": {
    "id": "9JWY8pShC5hZ"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "unique_values = df['housing_median_age'].unique()\n",
    "print(unique_values)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ifs6vXi6PGm0",
    "outputId": "128f201b-0e50-47ac-958d-9522e9436235"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# seaborn library is a powerful visualization tool built on top of matplotlib.\n",
    "# It provides a high-level interface for drawing attractive statistical graphics.\n",
    "import seaborn as sns\n",
    "\n",
    "# import the matplotlib.pyplot module and assign it to the alias 'plt'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize a new figure for the plot: 10 inches width and 6 inches height.\n",
    "plt.figure(figsize=(10, 6))\n",
    "#Histogram with Density Plot\n",
    "\"\"\"\n",
    ":\n",
    "sns.histplot(...): This function creates a histogram for the specified data.\n",
    "The bins parameter determines how many intervals will be used in the histogram.\n",
    "kde=True: enables the kernel density estimate (KDE) plot, which provides a smoothed version of the histogram.\n",
    " It shows the probability density function of the variable, helping to visualize the distribution more clearly.\n",
    "color='grey': sets the color of the histogram bars to grey.\n",
    "alpha=0.5: sets the transparency level of the histogram bars, making them semi-transparent (50% opacity), which can help in visualizing overlapping elements.\n",
    "\"\"\"\n",
    "sns.histplot(df['housing_median_age'], bins=int(df['housing_median_age'].max()) - int(df['housing_median_age'].min()), kde=True, color='grey', alpha=0.5)\n",
    "\n",
    "plt.title('Histogram with Density Plot')\n",
    "plt.xlabel('Housing median age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "ylT5gADDNmnv",
    "outputId": "7b4245c0-7f0a-458f-c264-bffbfdbf947b"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "unique_counts = df.nunique()\n",
    "print(\"Number of unique values:\\n\", unique_counts)\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kawsGB4MD6aZ",
    "outputId": "3b601166-5a15-45cc-ca4c-888645ea1b49"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "##🏆 Understanding the Data Challenge\n",
    "\n",
    "**Descriptive Statistics Calculation of NYC Taxi Trips**\n",
    "\n",
    "In this challenge, you will analyze a dataset to calculate descriptive statistics for both numeric and non-numeric columns.\n",
    "\n",
    "The goal of this challenge is to create a dictionary that stores descriptive statistics for each column in the dataset.\n",
    "\n",
    "This will include metrics for numeric columns (like min, max, mean, standard deviation, percentiles, and unique counts) and key statistics for non-numeric columns (like unique counts and the most common value).\n",
    "\n",
    "Finally, you will convert this information into a DataFrame for easier visualization.\n",
    "\n",
    "The data source will be the parquet file located at https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet\n",
    "\n",
    "\n",
    "Steps to Accomplish the Challenge\n",
    "\n",
    "- Load the Dataset:\n",
    "\n",
    "  Ensure you have the dataset loaded into a DataFrame named df.\n",
    "\n",
    "- Create a Results Dictionary:\n",
    "\n",
    "  Initialize an empty dictionary to store the results of your calculations.\n",
    "\n",
    "- Calculate Metrics for **Numeric** Columns:\n",
    "\n",
    "  Use the `select_dtypes()` method to create a DataFrame containing only numeric columns.\n",
    "\n",
    "  Iterate through the numeric columns and calculate the following metrics:\n",
    "  - Minimum value\n",
    "  - Maximum value\n",
    "  - Mean\n",
    "  - Standard deviation\n",
    "  - Count of non-null values\n",
    "  - Percentiles (25th, 50th, 75th)\n",
    "  - Unique count\n",
    "  - Most common value (mode)\n",
    "  \n",
    "- Calculate Metrics for **Non-Numeric** Columns:\n",
    "\n",
    "  Create a DataFrame for non-numeric columns using select_dtypes().\n",
    "  \n",
    "  Iterate through the non-numeric columns and calculate:\n",
    "  - Unique count\n",
    "  - Most common value (mode)\n",
    "  - Count of non-null values\n",
    "\n",
    "- Convert Results to a DataFrame:\n",
    "\n",
    "  Convert the results dictionary into a DataFrame for better visualization, using the `.T` attribute to transpose it.\n",
    "\n",
    "- Display the Results:\n",
    "\n",
    "  Print the resulting DataFrame to review the summary statistics for all columns.\n"
   ],
   "metadata": {
    "id": "Gp-OFqZRIPhj"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "df_taxi = pd.read_parquet(\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet\")\n"
   ],
   "metadata": {
    "id": "Sw9MBMPVJ_bC"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df_taxi.info()"
   ],
   "metadata": {
    "id": "TDST_5GtKGDC"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df_taxi.describe()"
   ],
   "metadata": {
    "id": "s38_t_zxKLAt"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Create a dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Calculate metrics for numeric columns\n",
    "numeric_df = df_taxi.select_dtypes(include='number')\n",
    "for column in numeric_df.columns:\n",
    "    results[column] = {\n",
    "        'min': numeric_df[column].min(),\n",
    "        'max': numeric_df[column].max(),\n",
    "        'mean': numeric_df[column].mean(),\n",
    "        'std': numeric_df[column].std(),\n",
    "        'count': numeric_df[column].count(),\n",
    "        'p90': numeric_df[column].quantile(0.25),\n",
    "        'p75': numeric_df[column].quantile(0.75),\n",
    "        'p50': numeric_df[column].quantile(0.5),\n",
    "        'p25': numeric_df[column].quantile(0.25),\n",
    "        'unique_count': numeric_df[column].nunique(),\n",
    "        'most_common': numeric_df[column].mode()[0] if not numeric_df[column].mode().empty else None,\n",
    "        'count_non_null': numeric_df[column].count(),\n",
    "    }\n",
    "\n",
    "# Calculate metrics for non-numeric columns\n",
    "non_numeric_df = df_taxi.select_dtypes(exclude='number')\n",
    "for column in non_numeric_df.columns:\n",
    "    results[column] = {\n",
    "        'unique_count': non_numeric_df[column].nunique(),\n",
    "        'most_common': non_numeric_df[column].mode()[0],\n",
    "        'count_non_null': non_numeric_df[column].count(),\n",
    "    }\n",
    "\n",
    "# Convert results to a DataFrame for better visualization\n",
    "# the .T attribute is used to transpose the DataFrame\n",
    "results_df = pd.DataFrame(results).T\n",
    "\n",
    "results_df\n"
   ],
   "metadata": {
    "id": "ILPhJDN-JfQ7"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "nunique_values_dict = {col: df_taxi[col].nunique() for col in df_taxi.columns}\n",
    "\n",
    "print(nunique_values_dict)"
   ],
   "metadata": {
    "id": "4U12gwFnKt9I"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "null_counts = df_taxi.isnull().sum()\n",
    "print(\"Number of null values:\\n\", null_counts)"
   ],
   "metadata": {
    "id": "qK6T2v31K8VL"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Cleaning\n",
    "Real-world datasets often have missing or inconsistent data. Pandas makes it easy to identify and clean this data.\n",
    "\n",
    "Make sure to always do a copy of your dataframe to be able to go back to the source.\n",
    "\n",
    "The way to do it is using the .**copy** function\n",
    "\n",
    "```python\n",
    "df_modified = df.copy()\n",
    "```"
   ],
   "metadata": {
    "id": "5D-xEpuO__Hf"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Renaming columns\n",
    "\n",
    "We can change the name of the columns of the dataframe using the **rename** function. This functions takes as parameter a dictionary with the old column names as key and the new column names as values.\n",
    "```python\n",
    "df.rename(columns='old_column_name1':'new_column_name1',\n",
    "  'old_column_name2':'new_column_name2'})\n",
    "```"
   ],
   "metadata": {
    "id": "JjwCCu9P4cDk"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df.columns\n",
    "df = df.rename(columns={'housing_median_age': 'median_house_age'})\n",
    "df.columns"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0RYq093f4bM8",
    "outputId": "db546b30-4108-48f6-bee8-332764aef15b"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Changing the format of columns\n",
    "\n"
   ],
   "metadata": {
    "id": "nppTj8yT7PH-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Using `astype()` function"
   ],
   "metadata": {
    "id": "8khYBVu373wW"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'A': ['1', '2', '3'],\n",
    "    'B': [10.5, 20.2, 30.3],\n",
    "    'C': [1.512, 2.023, 3.653],\n",
    "    'Date': ['2024-01-01', '2024-02-01', '2024-03-01'],\n",
    "    'Name': ['alice', 'bob', 'charlie'],\n",
    "    'Amount': [1500.5, 2500.75, 3000.45]\n",
    "})\n",
    "\n",
    "# Change column 'A' from string integer\n",
    "df['A'] = df['A'].astype(int)\n",
    "\n",
    "# Change column 'B' from float to integer\n",
    "df['B'] = df['B'].astype(int)\n",
    "\n",
    "df"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "cs40LbV-79MQ",
    "outputId": "3b4b3c54-6e66-4d93-d87c-4567010a75cd"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Rounding numeric columns\n"
   ],
   "metadata": {
    "id": "B_tNnAiI8Rwy"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df['C'] = df['C'].round(1)\n",
    "df"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "-TfV1dVn8uWH",
    "outputId": "de641252-3dae-4bfb-9dc5-9d41345baf95"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Formatting strings\n",
    "\n",
    "Applying lambda functions to the column for example to add 0´s to the left"
   ],
   "metadata": {
    "id": "nL5m60-f85Cv"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df['A'] = df['A'].apply(lambda x: f\"{x:03}\")\n",
    "df"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "I1HqecnT9M3L",
    "outputId": "c9582d4d-817b-4b5a-e11c-1c6e843b51f8"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Converting dates\n",
    "\n",
    "If you have date columns, you can use `pd.to_datetime()` to convert them to datetime format and `dt.strftime()` to display them in a specific format."
   ],
   "metadata": {
    "id": "DjHCJtgo9W32"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "qAYiV3XJ_Zq7"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['Date'] = df['Date'].dt.strftime('%a %d %B %Y')\n",
    "df\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "ihmTIOci9sqX",
    "outputId": "1517f5c8-a2c8-40db-bf09-12eacb9fd569"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Altering the text case\n",
    "For example we can choose to convert all the values in a string column to upper case."
   ],
   "metadata": {
    "id": "RnHqANEp-LiM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df['Name'] = df['Name'].str.upper()\n",
    "df"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "d5tEmESd-TpP",
    "outputId": "855e9314-0e35-444b-a44f-b0403d3fca2f"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Formatting currency\n",
    "If we are working with monetary values we might want to format the amount accordingly.\n"
   ],
   "metadata": {
    "id": "wAr49S6l-gym"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df['Amount_EUR'] = df['Amount'].apply(lambda x: f\"{x:,.2f} €\")\n",
    "df"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "zLmOiYj3-rsV",
    "outputId": "ac8d9248-ae93-479c-b282-8d36c416a3da"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Extracting parts of a string\n"
   ],
   "metadata": {
    "id": "b5OVT9AZATbN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df['Name_initial'] = df['Name'].str[0]\n",
    "df"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "i5zn-RbsAaF9",
    "outputId": "665c4f4a-d23d-4cfc-cf00-964bbe9f3bb7"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Handling Missing Values\n",
    "\n",
    "In case of a missing value we can fill it with an aggregated value like the mean of the column.\n",
    "\n",
    "- Initial Check:\n",
    "\n",
    "  Start with `df.isnull().sum()` to get a quick overview of null values in the dataframe\n",
    "\n",
    "- Detailed Analysis:\n",
    "\n",
    "  If you notice a significant number of missing values in certain columns, you might want to investigate those columns further. For example, you could check the data types and consider how to handle missing values based on the type of data (`mean` for numeric, `mode` for categorical, etc.).\n",
    "\n",
    "```python  \n",
    "# Check for missing values\n",
    "print(f\"Number of null values:\\n{df.isnull().sum()}\")\n",
    "\n",
    "# Fill missing values\n",
    "for column in df.columns:\n",
    "    if df[column].dtype in ['float64', 'int64']:  # Check if the column is numeric\n",
    "        df[column].fillna(df[column].mean(), inplace=True)  # Fill with mean\n",
    "    elif df[column].dtype == 'object':  # Check if the column is categorical\n",
    "        df[column].fillna(df[column].mode()[0], inplace=True)  # Fill with mode\n",
    "\n",
    "# Verify that there are no more missing values\n",
    "print(f\"Number of null values after filling:\\n{df.isnull().sum()}\")\n",
    "```\n"
   ],
   "metadata": {
    "id": "V6jpXneWAGLB"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "\n",
    "2. Discarding Null values\n",
    "\n",
    "In case of missing values we can also delete all the row.\n",
    "\n",
    "```python\n",
    "# Drop rows with missing values, if any\n",
    "df.dropna(inplace=True)\n",
    "```\n",
    "\n",
    "> 💡 When you´re doing cleanup task, it´s usually better to create a copy of the dataframe so that you can always go back to the initial version\n",
    "\n",
    "```python\n",
    "# Drop rows with missing values, if any without modifying the original dataframe\n",
    "df_clean = df.dropna()\n",
    "```"
   ],
   "metadata": {
    "id": "XlbVyr5QeHhk"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Date': ['2024-01-01', '2024-02-01', '2024-03-01'],\n",
    "    'Name': ['alice', 'bob', 'charlie'],\n",
    "    'Amount': [1500.5, 2500.75, np.nan]\n",
    "})"
   ],
   "metadata": {
    "id": "KXFej1H6BAoy"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df.isna()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "ETENGnPtBhFF",
    "outputId": "18a027fb-fa56-4dd8-9dd8-57c31ffe42f8"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df.isna().sum()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "pf3Bz7RzBjjn",
    "outputId": "4323aa04-1ee4-430e-9e48-010a53b68092"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df.isna().any(axis=1)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "VcDhKwm4CDx2",
    "outputId": "5ee7e9b1-5791-460e-ea7c-28af78a93527"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "#df.loc[] usues boolean indexing to select the rows from the dataframe that has any null\n",
    "df.loc[df.isna().any(axis=1)]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "id": "org1zONeBmvI",
    "outputId": "3a339796-4dd0-4f07-fee8-49147b9445b3"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Identify outliers\n",
    "\n",
    "**Outliers** are data points that differ significantly from the majority of the data. They could represent extreme values or rare events, but they could also be due to errors in data collection, measurement, or entry.\n",
    "\n",
    "There are several techniques to identify outliers in a DataFrame. The most common methods are:\n",
    "\n",
    "\n",
    "| Method                         | Description                                                                                   | When to Use                                                    |\n",
    "|---------------------------------|-----------------------------------------------------------------------------------------------|---------------------------------------------------------------|\n",
    "| **IQR (Interquartile Range)**   | Calculates Q1, Q3, and IQR; identifies outliers outside `Q1 - 1.5 * IQR` and `Q3 + 1.5 * IQR` | When you need to detect data points far from the central data |\n",
    "| **Z-Score**                     | Calculates the Z-score to find data points with high deviation from the mean                  | When data is normally distributed and outliers are extreme     |\n",
    "| **Boxplot**                     | Visualizes the distribution and outliers using quartiles and whiskers                         | For visual detection of outliers in univariate data           |\n",
    "| **Scatter Plot**                | Visualizes outliers in bivariate data by showing points far from clusters of data points      | For identifying outliers in two-variable relationships         |\n",
    "| **Pairplot (Multivariate)**     | Visualizes pairwise relationships in high-dimensional datasets                               | For multidimensional data and identifying outliers in multiple features |\n"
   ],
   "metadata": {
    "id": "mzEb1te7QRJM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "from vega_datasets import data\n",
    "cars = data.cars()\n",
    "\n",
    "# Display the first few rows to understand the data structure\n",
    "# print(cars.head())\n",
    "# Calculate Q1 (25th percentile) and Q3 (75th percentile) for the 'horsepower' column\n",
    "Q1 = cars['Horsepower'].quantile(0.25)\n",
    "Q3 = cars['Horsepower'].quantile(0.75)\n",
    "\n",
    "# Calculate IQR\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define bounds for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Identify outliers in the 'horsepower' column\n",
    "outliers = cars[(cars['Horsepower'] < lower_bound) | (cars['Horsepower'] > upper_bound)]\n",
    "\n",
    "# Display the outliers\n",
    "print(\"Outliers detected in 'horsepower' column:\")\n",
    "print(outliers[['Name', 'Horsepower']])\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xYb0sFnrQ3zh",
    "outputId": "b19de121-ba46-49bb-c9f7-c6e0a0f34c06"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "##🏆 Data Cleaning Challenge\n",
    "\n",
    "**Data Cleaning of Taxi Trip Data**\n",
    "\n",
    "In this challenge, you will work with a dataset of yellow taxi trip data to perform data cleaning tasks. Your objective is to handle missing values in the dataset appropriately, ensuring that it is ready for analysis.\n",
    "\n",
    "The data source will be the parquet file located at https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet\n",
    "\n",
    "Steps to Accomplish the Challenge\n",
    "\n",
    "- Load the Dataset:\n",
    "\n",
    "  Use the pandas library to read the parquet file from the provided URL and store it in a DataFrame named df_taxi.\n",
    "  \n",
    "- Check for Missing Values:\n",
    "\n",
    "  Print the number of null values in each column of the DataFrame to understand the extent of missing data.\n",
    "\n",
    "- Fill Missing Values:\n",
    "\n",
    "  Iterate through each column in the DataFrame:\n",
    "  - For numeric columns (of type `float64` or `int64`), fill missing values with the mean of that column.\n",
    "  - For categorical columns (of type `object`), fill missing values with the mode (most frequent value) of that column.\n",
    "\n",
    "- Verify Completion:\n",
    "\n",
    "  Print the number of null values in each column again to confirm that all missing values have been successfully filled.\n",
    "\n"
   ],
   "metadata": {
    "id": "eWKbXDWu_H32"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "df_taxi = pd.read_parquet(\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet\")"
   ],
   "metadata": {
    "id": "rm8xBNeDLEPH"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Check for missing values\n",
    "print(f\"Number of null values:\\n{df_taxi.isnull().sum()}\")"
   ],
   "metadata": {
    "id": "XXB5hwTrLVP-"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Fill missing values\n",
    "for column in df_taxi.columns:\n",
    "   if df_taxi[column].dtype in ['float64', 'int64']:  # Check if the column is numeric\n",
    "       df_taxi[column] = df_taxi[column].fillna(df_taxi[column].mean())  # Fill with mean\n",
    "   elif df_taxi[column].dtype == 'object':  # Check if the column is categorical\n",
    "       df_taxi[column] = df_taxi[column].fillna(df_taxi[column].mode()[0])  # Fill with mode\n",
    "\n",
    "# Verify that there are no more missing values\n",
    "print(f\"Number of null values after filling:\\n{df_taxi.isnull().sum()}\")"
   ],
   "metadata": {
    "id": "QMKHtewMrgHY",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7017db27-e0ea-40b5-ff2f-31e336b0845a"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Another way to fill the null values more flexible and scalable\n",
    "\n",
    "# Fill numeric columns with their mean and object columns with their mode using a dictionary\n",
    "fill_values = {col: df_taxi[col].mean() if df_taxi[col].dtype in ['float64', 'int64'] else df_taxi[col].mode()[0]\n",
    "               for col in df_taxi.columns}\n",
    "\n",
    "df_taxi = df_taxi.fillna(fill_values)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UqCdqCwuMlvI",
    "outputId": "0a53722e-96fc-4e8b-e036-75a50d6913dd"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Filtering and Selecting Data\n",
    "Pandas makes it easy to filter data based on certain conditions and select specific columns."
   ],
   "metadata": {
    "id": "-Cd9KhwOBpkZ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Selecting Columns\n",
    "We can use the **`[[]]`** syntax to indicate the columns to be selected.\n",
    "\n",
    "```python\n",
    "# Select specific columns\n",
    "subset = df[['Column1', 'Column2']]\n",
    "```\n",
    "\n",
    "Or we can use the **drop** function to remove the columns that we don´t want.\n",
    "\n",
    "```python\n",
    "df.drop(['Column3'], axis=1)\n",
    "```\n",
    "\n",
    "> Pandas DataFrames have 2 main axes:\n",
    "  - axis=0: Represents the rows (or index) of the DataFrame.\n",
    "  - axis=1: Represents the columns of the DataFrame."
   ],
   "metadata": {
    "id": "lhngC0fXBtTi"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Filtering Rows Based on Conditions\n",
    "\n",
    "We can select elements in the dataframe that meet a condition in this way:\n",
    "```python\n",
    "# Filter rows where 'Column1' > 50\n",
    "filtered_data = df[df['Column1'] > 50]\n",
    "```\n",
    "\n",
    "To apply multiple conditions when filtering a DataFrame in pandas, you should use the bitwise operators `&` (for AND) or `|` (for OR) instead of the `and` keyword. Additionally, each condition needs to be enclosed in parentheses\n",
    "\n",
    "```python\n",
    "# Filter rows where 'Column1' > 50 and at the same time 'Column2' > 0\n",
    "filtered_data = df[(df['Column1'] > 50) & (df['Column2']> 0)]\n",
    "```\n",
    "\n",
    "We can also achieve the same using the **query** operations\n",
    "\n",
    "```python\n",
    "# Filter rows where 'Column1' > 50\n",
    "filtered_data = df.query('Column1 > 50  & Column2 > 0' ]\n",
    "```\n"
   ],
   "metadata": {
    "id": "8-O8Rmzhd26E"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# We are going to get a dataframe\n",
    "# with longitude, latitude, median_house_value\n",
    "# where median_house_value is < 200000 and media housing_median_age > 30\n",
    "df_filtered = df[(df['median_house_value'] < 200000) & (df['housing_median_age'] > 30)]\n",
    "df_filtered = df_filtered[['longitude', 'latitude', 'median_house_value', 'housing_median_age']]\n",
    "df_filtered"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "mKf1fqNfCJJN",
    "outputId": "6c884446-adf0-4b9c-90de-a3fb2a67d338"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df_filtered = df.query('median_house_value < 200000 & housing_median_age > 30')[['longitude', 'latitude', 'median_house_value', 'housing_median_age']]\n",
    "df_filtered"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "adERHg9NCT83",
    "outputId": "22c7c0fd-02c2-4d23-a44b-6a83b264b737"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "When we filter rows in the dataframe, this makes the index to have jumps because of the removed elements.\n",
    "We can fix it with the **reset_index** command.\n",
    "Doing the reset_index moves the old index as a column of the dataframe by default, but as this information is not useful for our purposes we use the `drop=True`"
   ],
   "metadata": {
    "id": "zzZTrnfQHvzJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df_filtered = df_filtered.reset_index(drop=True)\n",
    "df_filtered"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sLSX3vEhIAHo",
    "outputId": "edd1c191-bf0e-4c8f-ba26-e3eeed6f8753"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Finding duplicates\n",
    "To check the data quality of the dataframe is very interesting to find any duplicated key element.\n",
    "\n",
    "To find duplicated rows we can use the **duplicated** function that returns a series fo boolean values indicating if the row is duplicated or not.\n",
    "\n",
    "```python\n",
    "df.loc[df.duplicated()]\n",
    "```"
   ],
   "metadata": {
    "id": "dFUW9wpMEI4r"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df.loc[df.duplicated()]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "c-WHYz_OEe8x",
    "outputId": "ae8af14b-ad7c-416a-8dc0-e6746448aa3b"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also look for rows where a certain column or combination of columns is duplicated."
   ],
   "metadata": {
    "id": "eXzk2u0TE2rF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df_duplicated = df.loc[df.duplicated(subset=['longitude','latitude'])]\n",
    "df_duplicated\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "gsyy7LGhE-OQ",
    "outputId": "948a24b9-970f-404d-ea3d-9e3e98304752"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "##🏆 Filtering and Selecting Data challenge\n",
    "\n",
    "**Filtering Taxi Trip Data**\n",
    "\n",
    "In this challenge, you will work with a dataset of yellow taxi trip data to filter out trips based on specific criteria related to passenger count and trip distance. Your objective is to create a new DataFrame that includes only valid taxi trips for further analysis.\n",
    "\n",
    "The data source will be the parquet file located at https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet\n",
    "\n",
    "Conditions:\n",
    "- the number of passengers is between 1 and 5 (inclusive)\n",
    "- the trip distance is greater than 0.\n",
    "\n",
    "The final result dataframe should include only the following columns:\n",
    "```\n",
    "'passenger_count', 'trip_distance', 'fare_amount', 'tip_amount', 'total_amount', 'VendorID','tpep_pickup_datetime', 'tpep_dropoff_datetime'\n",
    "```\n",
    "\n",
    "This will help ensure that the data you analyze is relevant and meets specific criteria.\n",
    "\n",
    "Steps to Accomplish the Challenge\n",
    "- Load the Dataset:\n",
    "\n",
    "  Ensure you have the yellow taxi trip dataset loaded into a DataFrame named df_taxi.\n",
    "\n",
    "- Understand the Data:\n",
    "\n",
    "  Familiarize yourself with the DataFrame's structure, including the columns and their data types, focusing on passenger_count and trip_distance.\n",
    "\n",
    "- Filter the Data:\n",
    "\n",
    "  Use boolean indexing to create a new DataFrame, filtered_data, that meets the following conditions:\n",
    "  The passenger_count must be greater than 0 and less than or equal to 5.\n",
    "  The trip_distance must be greater than 0.\n",
    "\n",
    "- Filter Columns:\n",
    "\n",
    "  Include only the columns that are relevant for the analysis.\n",
    "\n",
    "- Verify the Filtered Data:\n",
    "\n",
    "  Check the shape or the first few rows of the filtered_data DataFrame to ensure the filtering was applied correctly and that it contains only the desired trips.\n",
    "\n",
    "\n",
    "*passenger_count*>0 and passenger_count<=5"
   ],
   "metadata": {
    "id": "OVX8GFF8D0L1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "df_taxi = pd.read_parquet(\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet\")\n"
   ],
   "metadata": {
    "id": "sEzMlumENlyF"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "filtered_data = df_taxi[(df_taxi['passenger_count'] > 0) & (df_taxi['passenger_count'] <= 5) & (df_taxi['trip_distance'] > 0)]\n",
    "\n",
    "filtered_data = filtered_data[['passenger_count', 'trip_distance', 'fare_amount', 'tip_amount', 'total_amount', 'VendorID','tpep_pickup_datetime', 'tpep_dropoff_datetime']]\n",
    "print(filtered_data.head())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nUBmHe7BNduT",
    "outputId": "a5e201c5-f94c-4865-f1f8-474af7e085d1"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Grouping and Aggregating Data\n",
    "Grouping data is essential in data analytics to calculate summary statistics for different categories."
   ],
   "metadata": {
    "id": "KYdXAIDzFwvQ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Categorical values\n",
    "\n",
    "📝 A category column (or categorical column) in a dataset refers to a column that contains discrete values representing categories or groups rather than continuous numeric data.\n",
    "\n",
    "- Non-Numeric Data: The values in a categorical column are usually non-numeric. For example, values like \"red,\" \"blue,\" \"male,\" \"female,\" or \"low,\" \"medium,\" \"high\" are common.\n",
    "- Finite Set of Values: The possible values in a category column are usually finite and limited. For example, the \"color\" column might only contain values like \"red,\" \"green,\" or \"blue.\"\n",
    "- Labels or Groups: The values represent labels or groups for the data. These groups may or may not have an inherent order (ordinal or nominal).\n",
    "- Efficient Storage: In Pandas, categorical columns can be stored using a special Categorical type, which is more memory-efficient than using object types, especially with large datasets. https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.astype.html"
   ],
   "metadata": {
    "id": "u2SQBZowF5EM"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Benefits of `.astype('category')`\n",
    "- Memory efficiency: Significant reduction in memory usage by storing repeated values as integer codes.\n",
    "- Faster operations: Integer-based operations are much faster than string operations (e.g., grouping, sorting).\n",
    "- Efficient for repeated values: Ideal for columns with a small number of unique values but many rows.\n",
    "- Data integrity: You can define a limited set of categories and prevent invalid entries.\n",
    "- Support for ordinal data: Enables sorting and comparison based on the inherent order of categories.\n",
    "- Improved performance: Better performance for grouping, joining, and other operations involving categorical data."
   ],
   "metadata": {
    "id": "vc7GZe9EIRr4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from vega_datasets import data\n",
    "df_countries = data.countries()\n",
    "df_countries['country'] = df_countries['country'].astype('category')\n",
    "df = df_countries[['country','year', 'life_expect', 'fertility']]\n",
    "df"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 656
    },
    "id": "q0eUHcHuFxiJ",
    "outputId": "ed39c78a-027f-4fcd-bc42-3c50bffa286f"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Groupby\n",
    "We use `groupby` to group the DataFrame by the indicated column. Each unique value of it will create a separate group. For example, if there are groups for 1, 2, 3, etc., each of these groups will consist of rows.\n",
    "\n",
    "There are multiple aggregate functions that can be used:\n",
    "\n"
   ],
   "metadata": {
    "id": "t27PB9HWI5qK"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- `sum()`:Calculates the sum of the values for each group.\n",
    "```python\n",
    "grouped_data = df.groupby('CategoryColumn').sum().reset_index()\n",
    "```\n",
    "- `count()` :Counts the number of non-null values for each group.\n",
    "```python\n",
    "grouped_data = df.groupby('CategoryColumn').count().reset_index()\n",
    "```\n",
    "- `min()`:Finds the minimum value for each group.\n",
    "```python\n",
    "grouped_data = df.groupby('CategoryColumn').min().reset_index()\n",
    "```\n",
    "- `max()`: Finds the maximum value for each group.\n",
    "```python\n",
    "grouped_data = df.groupby('CategoryColumn').max().reset_index()\n",
    "```\n",
    "- `std()`: Calculates the standard deviation of the values for each group.\n",
    "```python\n",
    "grouped_data = df.groupby('CategoryColumn').std().reset_index()\n",
    "```\n",
    "- `median()`: Calculates the median value for each group.\n",
    "```python\n",
    "grouped_data = df.groupby('CategoryColumn').median().reset_index()\n",
    "```"
   ],
   "metadata": {
    "id": "SgpaCtvMd5Il"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#Playground : Use this cell to explore\n",
    "grouped_data = df.groupby('country').mean().reset_index()\n",
    "grouped_data"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 712
    },
    "id": "uuBfx62kJoxS",
    "outputId": "5cdfb945-d1cc-4055-8c92-945069ceef09"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "- `first()`: Returns the first value in each group.\n",
    "```python\n",
    "# Sort the DataFrame by 'Category' and 'Date'\n",
    "df_sorted = df.sort_values(by=['Category', 'Date'])\n",
    "# Group by 'Category' and get the first entry for each group\n",
    "first_entries = df_sorted.groupby('Category').first().reset_index()\n",
    "```\n",
    "- `last()`: Returns the last value in each group.\n",
    "\n"
   ],
   "metadata": {
    "id": "rqWeRqaoKtep"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df_sorted = df.sort_values(by=['country', 'year'])\n",
    "grouped_data = df_sorted.groupby('country').first().reset_index()\n",
    "grouped_data"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 712
    },
    "id": "m9T7-u_WDyp4",
    "outputId": "7515264d-8c56-47f6-adcc-34e81f94a3ab"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "- `agg()`:   The `agg()` function is used to specify the aggregation operations that will be applied to each group created by the `groupby()`. This allows you to calculate multiple statistics in a single step.\n",
    "\n",
    "  Inside `agg()`, each aggregation is defined in the format `new_column_name=('original_column_name', 'function')`.\n",
    "  This creates a new column named new_column_name in the resulting DataFrame.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> If you don’t add `reset_index()` after using `groupby()` and `agg()`, the result will be a DataFrame with a hierarchical index (multi-index) that consists of the grouping columns (in this case, passenger_count). This means that the grouping column(s) will become the index of the resulting DataFrame rather than regular columns.\n",
    "\n",
    "**Implications of Not Using reset_index()**\n",
    "\n",
    "- Accessing Data: You will have to use the `.loc[]` or `.iloc[]` methods to access the data, which can be less intuitive than working with a standard DataFrame.\n",
    "\n",
    "- DataFrame Appearance: The DataFrame will look different because the columns used to group by will be part of the index rather than a column, which may make it harder to interpret at a glance.\n",
    "\n",
    "- Subsequent Operations: If you want to perform further operations that expect the grouping variable to be a column (like merging, filtering, or plotting), you may need to reset the index later on, which can add extra steps to your workflow."
   ],
   "metadata": {
    "id": "t1rBk68JDpWS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df.head()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "kn_xjXMgMKiz",
    "outputId": "4ce35942-835e-4e73-f4dc-c5a7329e368e"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Use this as a playground to explore with grouping and aggregations\n",
    "\n",
    "df_summary = df.groupby('country').agg({\n",
    "    'year': ['min', 'max'],           # First and last year with data\n",
    "    'life_expect': 'mean',            # Average GDP\n",
    "    'fertility': 'median'             # Median fertility\n",
    "})\n",
    "df_summary"
   ],
   "metadata": {
    "id": "u_toQZuNTFMI",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "outputId": "20b49785-ac94-484e-c39e-d126b2fa6593"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "##🏆 Grouping and Aggregating Data Challenge\n",
    "\n",
    "**Does the number of passengers affect the tip amount for the taxi trip?**\n",
    "\n",
    "You are tasked with analyzing taxi trip data to understand how passenger count affects tipping behavior and trip characteristics.\n",
    "\n",
    "Using the filtered DataFrame from the previous challenge, calculate the average tip amount, average trip distance, and the total number of trips for different passenger counts.\n",
    "\n",
    "- The average tip amount for each group of passenger counts.\n",
    "- The average trip distance for each group.\n",
    "- The total number of trips for each group.\n",
    "\n",
    "By completing this challenge, you will gain insights into the relationship between passenger count and trip metrics, which can be valuable for understanding customer behavior and improving service.\n",
    "\n",
    "Steps to Accomplish the Challenge\n",
    "- Import Required Libraries:\n",
    "\n",
    "  Ensure you have the necessary libraries imported, particularly pandas.\n",
    "\n",
    "- Load the Data:\n",
    "\n",
    "  Load the taxi trip data into a DataFrame. If using a filtered DataFrame (e.g., filtered_data), make sure it has already been created based on relevant criteria (such as valid passenger counts).\n",
    "\n",
    "- Group the Data:\n",
    "\n",
    "  Use the groupby() method on the passenger_count column to group the data accordingly.\n",
    "\n",
    "- Aggregate the Statistics:\n",
    "\n",
    "  Use the agg() function to compute:\n",
    "  - The average of the tip_amount column (average tip).\n",
    "  - The average of the trip_distance column (average trip distance).\n",
    "  - The count of non-null values (number of trips).\n",
    "\n",
    "  Ensure to name the resulting columns appropriately.\n",
    "\n",
    "- Reset the Index:\n",
    "\n",
    "  Call reset_index() to convert the grouped object back into a standard DataFrame format, making it easier to read and manipulate.\n",
    "\n",
    "- Display the Results:\n",
    "\n",
    "  Print or display the resulting DataFrame average_stats to view the aggregated statistics."
   ],
   "metadata": {
    "id": "mR6gIbnpUeMs"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "average_stats = filtered_data.groupby('passenger_count').agg(\n",
    "    average_tip=('tip_amount', 'mean'),\n",
    "    average_trip_distance=('trip_distance', 'mean'),\n",
    "    number_trips=('tip_amount', 'count')\n",
    ").reset_index()\n",
    "# The reset_index() method is called to convert the grouped object back into a DataFrame.\n",
    "average_stats"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "1akwAm7aNzYS",
    "outputId": "f952b961-4064-48d7-f268-802a57e8b490"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Visualization with Pandas\n",
    "You can create quick plots directly from pandas, which integrates well with Matplotlib for more complex visualizations.\n",
    "\n",
    "For **Feature understanding**, also called univariate analysis we use:\n",
    "- Histogram\n",
    "- KDE\n",
    "- Boxplot\n",
    "\n",
    "For **Feature relationships**, also known as multivariate analysis the most frequent chart types are:\n",
    "- Scatterplot\n",
    "- Heatmap correlation\n",
    "- Pairplot\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "BJee58SddlZd"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Matplotlib summary\n",
    "\n",
    "- Line Chart: Used to display data points connected by straight lines, ideal for showing trends over time. `plt.plot(x, y)`\n",
    "- Bar Chart: Displays data with rectangular bars representing the values, useful for comparing different categories. `plt.bar(x, height)`\n",
    "- Horizontal Bar Chart: Similar to a bar chart, but bars are displayed horizontally. `plt.barh(y, width)`\n",
    "- Histogram: Used to represent the distribution of numerical data by dividing the data into bins. `plt.hist(data, bins=10)`\n",
    "- Scatter Plot: Displays individual data points using Cartesian coordinates, useful for showing relationships between two variables.\n",
    "`plt.scatter(x, y)`\n",
    "- Pie Chart: Represents proportions of a whole as slices of a circle, suitable for showing percentage distributions.\n",
    "`plt.pie(sizes, labels=labels)`\n",
    "- Box Plot: Displays the distribution of data based on five summary statistics: minimum, first quartile, median, third quartile, and maximum. `plt.boxplot(data)`\n",
    "- Heatmap: Represents data values in a matrix format using color gradients, useful for visualizing correlations or patterns. `plt.imshow(data, cmap='hot', interpolation='nearest')`\n",
    "- Area Chart: Similar to line charts, but the area below the line is filled in, highlighting the volume of data. `plt.fill_between(x, y)`\n",
    "- Violin Plot: Combines box plot and kernel density plot to show data distribution and density, useful for comparing distributions between multiple groups. `plt.violinplot(data)`\n",
    "\n",
    "\n",
    "[Documentation](https://matplotlib.org/)\n",
    "\n",
    "![Matplotlib cheat sheet](https://matplotlib.org/cheatsheets/_images/cheatsheets-1.png)(https://matplotlib.org/cheatsheets/)\n",
    "\n",
    "![Matplotlib cheat sheet](https://matplotlib.org/cheatsheets/_images/cheatsheets-2.png)(https://matplotlib.org/cheatsheets/)\n"
   ],
   "metadata": {
    "id": "Ak82Q7T2NX3y"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###  Simple Line Plot\n",
    "\n",
    "A simple line chart is best used in the following scenarios:\n",
    "\n",
    "- **Time Series Data**: When you want to visualize trends over time, such as daily, monthly, or yearly data. Line charts effectively show how values change over intervals.\n",
    "\n",
    "- **Continuous Data**: Ideal for displaying continuous data where you expect smooth transitions between points, such as temperature changes, stock prices, or sales figures.\n",
    "\n",
    "- **Comparison of Multiple Series**: When you need to compare multiple related datasets over the same time period. Multiple lines can be plotted on the same chart to show how different groups compare over time.\n",
    "\n",
    "- **Highlighting Trends**: Use line charts to identify upward or downward trends and cycles in the data, helping to visualize long-term trends effectively.\n",
    "\n",
    "- **Simple Relationships**: When you want to illustrate the relationship between two continuous variables (e.g., time and temperature) without the need for more complex visualizations.\n",
    "\n",
    "Overall, a simple line chart is a powerful and straightforward tool for presenting data in a way that highlights changes, trends, and comparisons clearly.\n",
    "\n",
    "\n",
    "```python\n",
    "# Plot a line chart for a time series\n",
    "df['DateColumn'] = pd.to_datetime(df['DateColumn'])\n",
    "df.set_index('DateColumn')['NumericalColumn'].plot(title='Time Series Plot')\n",
    "```\n"
   ],
   "metadata": {
    "id": "KH01xGxpYW5p"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Generate sample data for a month\n",
    "date_range = pd.date_range(start='2024-01-01', end='2024-01-31')\n",
    "np.random.seed(0)  # For reproducibility\n",
    "numerical_values = np.random.randint(0, 100, size=len(date_range))\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'DateColumn': date_range,\n",
    "    'NumericalColumn': numerical_values\n",
    "})\n",
    "\n",
    "# Convert 'DateColumn' to datetime\n",
    "df['DateColumn'] = pd.to_datetime(df['DateColumn'])\n",
    "\n",
    "# Plot a line chart for the time series\n",
    "df.set_index('DateColumn')['NumericalColumn'].plot(title='Time Series Plot for January 2024')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "\n",
    "# Adjust the y-axis to start at 0\n",
    "plt.ylim(0, df['NumericalColumn'].max()+10)  # Set the y-axis limits\n",
    "\n",
    "plt.grid()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "id": "NARlGpccW25i",
    "outputId": "33abe9af-39f8-4ac2-ba33-19a8ed8b8765"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 🏆 Line Chart Challenge\n",
    "\n",
    "**Analyzing Daily Taxi Trip Data**\n",
    "\n",
    "The objective of this challenge is to analyze and visualize daily taxi trip data for January 2024. You will calculate the number of trips taken each day and create a time series plot to display this information clearly.\n",
    "\n",
    "Steps to Accomplish the Challenge:\n",
    "- Import Necessary Libraries:\n",
    "- Load Your Data: Load the taxi trip data into a DataFrame.You can use the data generated in the previous challenge.\n",
    "- Convert Timestamp to Datetime:\n",
    "Convert the tpep_pickup_datetime column to a datetime format for easier manipulation.\n",
    "- Extract the Date: Create a new column called date_pickup that contains only the date part of the tpep_pickup_datetime.\n",
    "- Group Data by Date: Group the data by the date_pickup column and calculate the number of trips (using the count of tip_amount) for each day.\n",
    "- Create a Time Series Plot: Plot the number of trips against the date using a line chart. Set the date_pickup column as the index.\n",
    "- Customize the Plot: Add labels for the x-axis and y-axis.\n",
    "Set the x-axis limits to display only the data for January 2024.\n",
    "Rotate x-axis labels to 90 degrees for better readability.\n",
    "Enable grid lines for better readability and display the plot.\n"
   ],
   "metadata": {
    "id": "kFN1gAUvCrhP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Convert the 'timestamp' column to datetime\n",
    "filtered_data['tpep_pickup_datetime'] = pd.to_datetime(filtered_data['tpep_pickup_datetime'])\n",
    "\n",
    "# Extract the date from the timestamp\n",
    "filtered_data['date_pickup'] = filtered_data['tpep_pickup_datetime'].dt.date\n",
    "\n",
    "average_daily_stats = filtered_data.groupby('date_pickup').agg(\n",
    "    number_trips=('tip_amount', 'count')\n",
    ").reset_index()\n",
    "\n",
    "\n",
    "\n",
    "# Plot a line chart for the time series\n",
    "average_daily_stats.set_index('date_pickup')['number_trips'].plot(title='Time Series Plot for January 2024')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Trips')\n",
    "# Set the x-axis limit to show a specific date range\n",
    "plt.xlim(pd.Timestamp('2024-01-01'), pd.Timestamp('2024-01-31'))  # Adjust dates as needed\n",
    "# Rotate x-axis labels to 90 degrees\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.grid()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "id": "jGl70EuzOAzc",
    "outputId": "f9b92bfb-cab4-4de1-8d32-ed29c53785a9"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bar Plot for Categorical Data\n",
    "\n",
    "Bar plot charts are particularly useful in the following scenarios:\n",
    "\n",
    "- **Categorical Data Comparison**: When you need to compare quantities across different categories. Bar plots clearly display the differences in counts or measurements among distinct groups.\n",
    "\n",
    "- **Discrete Variables**: Ideal for visualizing discrete data where categories are non-numerical, such as survey responses (e.g., preferences, types of products, etc.).\n",
    "\n",
    "- **Ranking**: Bar charts can effectively show the ranking of categories based on their values. For example, you might use a bar plot to display sales figures for different products, making it easy to see which product performed best.\n",
    "\n",
    "- **Frequency Distribution**: When you want to show the frequency distribution of a categorical variable, such as the number of occurrences of different species in a dataset.\n",
    "\n",
    "- **Multiple Series Comparison**: Bar plots can display multiple datasets side-by-side (grouped bar plots) or stacked, allowing for easy comparison of different categories across multiple groups (e.g., sales by region and by product).\n",
    "\n",
    "- **Data Presentation**: When you need a clear and straightforward way to present data in reports or presentations, bar plots are easily understood and visually impactful.\n",
    "\n",
    "- **Visualization of Changes Over Time** (for Categorical Data): While line charts are generally preferred for continuous data over time, bar plots can also effectively display changes in categorical data over discrete time intervals (e.g., monthly sales by category).\n",
    "\n",
    "Overall, bar plots are a versatile tool for visualizing categorical data and comparing quantities, making them a staple in data analysis and presentation.\n",
    "\n",
    "```python\n",
    "# Plot a bar chart for the counts of each category\n",
    "df['CategoryColumn'].value_counts().plot(kind='bar', title='Category Counts')\n",
    "```\n"
   ],
   "metadata": {
    "id": "B7vlAt4sYbVd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data for categorical counts\n",
    "data = {\n",
    "    'CategoryColumn': ['A', 'B', 'A', 'C', 'B', 'A', 'C', 'B', 'A', 'D', 'D', 'C']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Count occurrences of each category\n",
    "category_counts = df['CategoryColumn'].value_counts()\n",
    "\n",
    "# Plot a bar chart for the counts of each category\n",
    "plt.figure(figsize=(10, 6))  # Set the figure size\n",
    "category_counts.plot(kind='bar', color='skyblue', edgecolor='black', title='Category Counts')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)  # Rotate x-axis labels for better readability\n",
    "\n",
    "# Adding a grid for easier reading\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Display the value on top of each bar\n",
    "for index, value in enumerate(category_counts):\n",
    "    plt.text(index, value, str(value), ha='center', va='bottom')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()  # Adjust layout for better spacing\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "pREjhWoaZDGl",
    "outputId": "07559621-7cf8-481d-a41d-f5304df380ce"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "###🏆 Bar Chart Challenge\n",
    "\n",
    "**Analyzing Taxi Trip Volume by Vendor**\n",
    "\n",
    "The objective of this challenge is to analyze and visualize the volume of taxi trips per vendor using the provided dataset from the Filtering and Selecting Data Challenge. You will count the number of trips associated with each vendor and create a bar chart to display this information clearly.\n",
    "\n",
    "Steps to Accomplish the Challenge:\n",
    "- Import Necessary Libraries\n",
    "- Load Your Data\n",
    "- Count Trip Occurrences per Vendor\n",
    "- Create a Bar Chart\n",
    "- Add Labels and Titles: Label the x-axis as \"Vendor ID\" and the y-axis as \"Trips\". Rotate x-axis labels if necessary for better readability.\n",
    "- Enhance Readability with Grid Lines\n",
    "- Display Values on Top of Bars\n",
    "- Show the Plot\n",
    "\n",
    "By following these steps, you will generate a bar chart that visually represents the volume of taxi trips for each vendor. This visualization will help identify trends in trip distribution among different vendors and provide insights into their relative performance."
   ],
   "metadata": {
    "id": "9mEUyY2iFoca"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count occurrences of each category\n",
    "category_counts = filtered_data['VendorID'].value_counts()\n",
    "\n",
    "# Plot a bar chart for the counts of each category\n",
    "plt.figure(figsize=(10, 6))  # Set the figure size\n",
    "category_counts.plot(kind='bar', color='lightgrey', edgecolor='black', title='Trips per vendor')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Vendor ID')\n",
    "plt.ylabel('Trips')\n",
    "plt.xticks(rotation=0)  # Rotate x-axis labels for better readability\n",
    "\n",
    "# Adding a grid for easier reading\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Display the value on top of each bar\n",
    "for index, value in enumerate(category_counts):\n",
    "    plt.text(index, value, str(round(value / 1_000))+' thousands', ha='center', va='bottom')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()  # Adjust layout for better spacing\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "id": "cU7Op-ttOef3",
    "outputId": "fa7bb6cb-422a-4ddc-d40a-33fd3d0fb45d"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Scatter Plot for Numerical Relationships\n",
    "\n",
    "Scatter plots are useful in the following scenarios:\n",
    "\n",
    "- **Relationship Between Two Variables**: When you want to explore the relationship or correlation between two numerical variables. Scatter plots can reveal whether a positive, negative, or no correlation exists.\n",
    "\n",
    "- **Identifying Trends**: They help visualize trends in data, showing how one variable changes in relation to another. For example, you might use a scatter plot to analyze how temperature affects ice cream sales.\n",
    "\n",
    "- **Outlier Detection**: Scatter plots are effective for identifying outliers or anomalies in the data. Points that fall far away from the general cluster of data can indicate unusual behavior or errors in data collection.\n",
    "\n",
    "- **Distribution Visualization**: When you need to see the distribution of data points across two dimensions. This can help in understanding the density and spread of data.\n",
    "\n",
    "- **Multivariable Analysis**: If you want to explore potential interactions between two variables while considering other variables, scatter plots can be enhanced with color or size encoding to represent additional data dimensions.\n",
    "\n",
    "- **Comparative Analysis**: They can be used to compare different groups or categories within the data. By using different colors or markers for different categories, you can see how they relate to each other.\n",
    "\n",
    "- **Regression Analysis**: Scatter plots are often the first step in regression analysis, allowing you to visualize the data before fitting a regression model to understand the relationship better.\n",
    "\n",
    "In summary, scatter plots are a powerful tool for visualizing relationships and distributions in numerical data, making them invaluable for exploratory data analysis and hypothesis testing.\n",
    "\n",
    "```python\n",
    "# Plot a scatter plot to see relationships between two numerical columns\n",
    "df.plot(kind='scatter', x='NumericalColumn1', y='NumericalColumn2', title='Scatter Plot')\n",
    "```"
   ],
   "metadata": {
    "id": "PWVAghsBYeUk"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Sample data for two numerical columns\n",
    "np.random.seed(0)  # For reproducibility\n",
    "data = {\n",
    "    'NumericalColumn1': np.random.rand(100) * 100,  # Random values between 0 and 100\n",
    "    'NumericalColumn2': np.random.rand(100) * 100  # Random values between 0 and 100\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create a scatter plot to see relationships between two numerical columns\n",
    "plt.figure(figsize=(10, 6))  # Set the figure size\n",
    "plt.scatter(df['NumericalColumn1'], df['NumericalColumn2'],\n",
    "            color='blue', alpha=0.6, edgecolor='black')\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Scatter Plot of NumericalColumn1 vs NumericalColumn2')\n",
    "plt.xlabel('NumericalColumn1')\n",
    "plt.ylabel('NumericalColumn2')\n",
    "\n",
    "# Adding a grid for better readability\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Optionally, you can add a regression line (if applicable)\n",
    "# Here, we calculate a simple linear regression line\n",
    "m, b = np.polyfit(df['NumericalColumn1'], df['NumericalColumn2'], 1)\n",
    "plt.plot(df['NumericalColumn1'], m * df['NumericalColumn1'] + b, color='red', linewidth=2)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()  # Adjust layout for better spacing\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "64ClJ7T5Z5mI",
    "outputId": "576f1841-54c0-47b6-8db5-45735326af31"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 🏆 Scatterplot Chart Challenge\n",
    "\n",
    "**Analyzing Trip Data Relationships**\n",
    "\n",
    "The goal of this challenge is to analyze the relationship between the total amount charged for taxi trips and the distance traveled. This involves cleaning the dataset by removing outliers and correcting data issues, followed by visualizing the results with a scatter plot and a regression line.\n",
    "\n",
    "High-Level Steps:\n",
    "\n",
    "- Data Cleaning:\n",
    "\n",
    "  - Identify and filter out outlier trips with excessive trip distances (greater than 200 kilometers).\n",
    "  - Convert any negative values in the total_amount column to their absolute values to ensure all amounts are non-negative.\n",
    "\n",
    "- Data Visualization:\n",
    "\n",
    "  - Create a scatter plot to visualize the relationship between total_amount and trip_distance.\n",
    "  - Set appropriate titles and labels for clarity, including a grid for better readability.\n",
    "\n",
    "- Regression Analysis:\n",
    "\n",
    "  - Calculate the linear regression line to understand the correlation between the two variables. Use `numpy.polyfit()` to obtain the slope and intercept.\n",
    "  - Generate a range of x-values for the regression line based on the cleaned total_amount data.\n",
    "  - Calculate the corresponding y-values using the linear regression equation.\n",
    "\n",
    "- Final Visualization:\n",
    "\n",
    "  Plot the regression line over the scatter plot to illustrate the relationship between the total amount charged and trip distance.\n",
    "  Display the final plot with proper layout adjustments for optimal viewing.\n",
    "\n",
    "This challenge will help you practice data cleaning, visualization techniques, and regression analysis using Python and its libraries."
   ],
   "metadata": {
    "id": "661xm3SdK-yU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#We identify some outliers and data issues that we need to correct:\n",
    "\n",
    "#There are some outlier trips with trip_distances of thousands of kilometers\n",
    "filtered_data = filtered_data[(filtered_data['trip_distance'] < 200)]\n",
    "#There are also trips with negative value in total_amount\n",
    "filtered_data['total_amount'] = filtered_data['total_amount'].abs()\n",
    "\n",
    "\n",
    "# Create a scatter plot to see relationships between two numerical columns\n",
    "plt.figure(figsize=(10, 6))  # Set the figure size\n",
    "plt.scatter(filtered_data['total_amount'], filtered_data['trip_distance'],\n",
    "            color='blue', alpha=0.6, edgecolor=(0, 0, 0, 0))\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Scatter Plot of Total amount vs Trip distance')\n",
    "plt.xlabel('Total amount')\n",
    "plt.ylabel('Trip distance')\n",
    "\n",
    "# Adding a grid for better readability\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Calculate linear regression\n",
    "m, b = np.polyfit(filtered_data['total_amount'], filtered_data['trip_distance'], 1)\n",
    "\n",
    "# Create a range of x values for the regression line\n",
    "x_values = np.linspace(filtered_data['total_amount'].min(), filtered_data['total_amount'].max(), 100)\n",
    "\n",
    "# Calculate the corresponding y values for the regression line\n",
    "y_values = m * x_values + b\n",
    "\n",
    "# Plot the regression line\n",
    "plt.plot(x_values, y_values, color='red', linewidth=2)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()  # Adjust layout for better spacing\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 652
    },
    "id": "7eZMaG4pOwow",
    "outputId": "e359c7c5-3c1c-4135-ba5d-c577a2e77b9e"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Advanced Data Analytics: Calculating Correlations\n",
    "Correlation analysis helps identify relationships between numerical columns, often a key part of data analytics.\n",
    "\n",
    "```python\n",
    "\n",
    "# Calculate correlations between numerical columns\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Display the correlation matrix\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Plot a heatmap (requires seaborn library)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n",
    "```\n"
   ],
   "metadata": {
    "id": "xP8AhPcodTKE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data creation\n",
    "np.random.seed(0)  # For reproducibility\n",
    "\n",
    "# Creating a DataFrame with realistic numerical data\n",
    "data = {\n",
    "    'Sales': np.random.normal(200, 50, 1000),  # Normal distribution of sales\n",
    "    'Advertising': np.random.normal(50, 10, 1000),  # Advertising expenses\n",
    "    'Profit': np.random.normal(30, 5, 1000),  # Profit values\n",
    "    'Customer_Rating': np.random.uniform(1, 5, 1000),  # Customer ratings from 1 to 5\n",
    "    'Product_Cost': np.random.normal(20, 5, 1000)  # Cost of the product\n",
    "}\n",
    "\n",
    "# Adjusting profit to be related to sales and advertising\n",
    "data['Profit'] = data['Sales'] * 0.15 - data['Product_Cost'] + np.random.normal(0, 5, 1000)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate correlations between numerical columns\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Display the correlation matrix\n",
    "print(\"Correlation Matrix:\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Plot a heatmap\n",
    "plt.figure(figsize=(10, 8))  # Set the figure size\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", square=True, linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 942
    },
    "id": "AWHVOYcrae7B",
    "outputId": "ec360480-fed7-4ac8-e5cb-9a625736545a"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Applying Functions to Columns in DataFrames\n",
    "In data analytics, applying functions to columns of a DataFrame is a powerful technique for transforming and manipulating data. This can be useful for a variety of scenarios, such as data cleaning, feature engineering, and deriving new insights from existing data.\n",
    "\n",
    "Here, we will explore several examples and use cases to illustrate how to apply functions to columns in a pandas DataFrame.\n"
   ],
   "metadata": {
    "id": "uyGYfO7Qdb8f"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1. Squaring Values in a Numerical Column**\n",
    "\n",
    "Use Case: This transformation could be used in a scenario where you want to create polynomial features for a regression model.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "Q9FlPLa1Lf6r"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = pd.DataFrame({\n",
    "    'NumericalColumn': [1, 2, 3, 4, 5]\n",
    "})\n",
    "\n",
    "# Apply a custom function to square the values in 'NumericalColumn'\n",
    "data['SquaredValues'] = data['NumericalColumn'].apply(lambda x: x ** 2)\n",
    "\n",
    "print(data)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-zxZjHLmMiZj",
    "outputId": "6718aa35-318a-44b7-9866-7c2ab8ab80ee"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2. Converting Units**\n",
    "\n",
    "Use Case: This is useful in scenarios where temperature data needs to be standardized to a specific unit for analysis or visualization.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "SVlTPc6BMe56"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Sample data with temperature in Celsius\n",
    "temperature_data = pd.DataFrame({\n",
    "    'Celsius': [0, 20, 100, -10]\n",
    "})\n",
    "\n",
    "# Function to convert Celsius to Fahrenheit\n",
    "def celsius_to_fahrenheit(celsius):\n",
    "    return (celsius * 9/5) + 32\n",
    "\n",
    "# Apply the conversion function\n",
    "temperature_data['Fahrenheit'] = temperature_data['Celsius'].apply(celsius_to_fahrenheit)\n",
    "\n",
    "print(temperature_data)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xRICfzHzMZ6d",
    "outputId": "573da9dc-6b94-4eca-9c96-687f97fac146"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**3. Text Processing**\n",
    "\n",
    "Use Case: This can be particularly useful in data cleaning, where inconsistent capitalization in text data needs to be standardized before further analysis or reporting.\n"
   ],
   "metadata": {
    "id": "h667M5q9MWc4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "# Sample data with names\n",
    "name_data = pd.DataFrame({\n",
    "    'Names': ['alice', 'BOB', 'Charlie', 'dave']\n",
    "})\n",
    "\n",
    "# Apply a function to capitalize the first letter of each name\n",
    "name_data['CapitalizedNames'] = name_data['Names'].apply(lambda x: x.capitalize())\n",
    "\n",
    "print(name_data)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "etGazRkDMMNB",
    "outputId": "b6b0d097-fc73-4bc5-d68e-21593d384b4b"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**4. Conditional Transformations**\n",
    "\n",
    "Use Case: This transformation can help categorize numerical values into discrete categories, which is useful for summarizing performance metrics.\n"
   ],
   "metadata": {
    "id": "qX3H2270MJFi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "# Sample data with scores\n",
    "scores_data = pd.DataFrame({\n",
    "    'Scores': [85, 92, 78, 90, 60]\n",
    "})\n",
    "\n",
    "# Apply a function to categorize scores\n",
    "scores_data['Grade'] = scores_data['Scores'].apply(lambda x: 'A' if x >= 90 else ('B' if x >= 80 else 'C'))\n",
    "\n",
    "print(scores_data)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ylwmu2HMBY7",
    "outputId": "1ae3238a-6c01-4236-942a-b66994af0779"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**5. Date Manipulation**\n",
    "\n",
    "Use Case: Extracting specific date components is crucial for time series analysis and allows for grouping or filtering by time periods.\n"
   ],
   "metadata": {
    "id": "XVNDTzTKLi-e"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HAzi3SNWVYrh",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e603458f-29d0-4996-934e-0da1643b9dd1"
   },
   "source": [
    "\n",
    "# Sample data with dates\n",
    "date_data = pd.DataFrame({\n",
    "    'Date': ['2023-01-01', '2023-06-15', '2023-09-20']\n",
    "})\n",
    "\n",
    "# Convert to datetime format and extract the month\n",
    "date_data['Date'] = pd.to_datetime(date_data['Date'])\n",
    "date_data['Month'] = date_data['Date'].apply(lambda x: x.month)\n",
    "\n",
    "print(date_data)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "92vu7kDgV0TU"
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
