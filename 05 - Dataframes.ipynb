{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "authorship_tag": "ABX9TyNxE85ZFfeVCDELBL8Kmq7F"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Key Statistical Concepts\n",
    "\n",
    "\n",
    "### Mean\n",
    "The mean is the average of a dataset and is calculated by summing all the values and dividing by the total number of observations. It provides a central value of the data.\n",
    "\n",
    "- Example:\n",
    "\n",
    "In a data analytics scenario, suppose we have the following sales data for a week (in thousands of dollars): [10, 15, 20, 25, 30]. The mean or average will be 20\n",
    "\n",
    "```python\n",
    "sales = [10, 15, 20, 25, 30]\n",
    "mean_sales = sum(sales) / len(sales)\n",
    "print(f\"Mean Sales:{mean_sales}\") # Output: Mean Sales: 20.0\n",
    "```\n",
    "### Median\n",
    "The median is the middle value of a dataset when sorted in ascending order. It is useful in datasets that may contain outliers.\n",
    "\n",
    "- Example:\n",
    "\n",
    "Consider customer ages: [22,25,29,50,35], the median age, the one in the middle is 29\n",
    "\n",
    "```python\n",
    "ages = [22, 25, 29, 50, 35]\n",
    "sorted_ages = sorted(ages)\n",
    "n = len(sorted_ages)\n",
    "median_age = sorted_ages[n // 2] if n % 2 != 0 else (sorted_ages[n // 2 - 1] + sorted_ages[n // 2]) / 2\n",
    "print(f\"Median Age: {median_age}\") # Output: Median Age: 29\n",
    "```\n",
    "\n",
    "### Mode\n",
    "The mode is the value that appears most frequently in a dataset. It is particularly useful for categorical data.\n",
    "\n",
    "Example:\n",
    "In a survey about favorite fruits, the responses are:\n",
    "[\"Apple\",\"Banana\",\"Apple\",\"Orange\",\"Banana\",\"Banana\"]\n",
    "The mode, or the most frequent value is \"Banana\"\n",
    "\n",
    "```python\n",
    "from statistics import mode\n",
    "\n",
    "fruits = [\"Apple\", \"Banana\", \"Apple\", \"Orange\", \"Banana\", \"Banana\"]\n",
    "mode_fruit = mode(fruits)\n",
    "print(f\"Mode of Fruits:{mode_fruit}\") # Output: Mode of Fruits: Banana\n",
    "```\n",
    "### Variance\n",
    "Variance measures how far each number in the dataset is from the mean and indicates the degree of spread in the data.\n",
    "\n",
    "- Example:\n",
    "\n",
    "In analyzing test scores: [80,85,90,95,100]\n",
    "\n",
    "Mean =\n",
    "$\\frac{80 + 85 + 90 + 95 + 100}{5}$ = $\\frac{450}{5}$ = 90\n",
    "\n",
    "Calculate the Squared Differences from the Mean:\n",
    "\n",
    "- $(80-90)^2 = 100$\n",
    "- $(85-90)^2 = 25$\n",
    "- $(90-90)^2 = 0$\n",
    "- $(95-90)^2 = 25$\n",
    "- $(100-90)^2 = 100$\n",
    "\n",
    "Sum the Squared Differences:\n",
    "100+25+0+25+100=250\n",
    "\n",
    "Calculate the Variance (using sample variance, which divides by $n‚àí1$):\n",
    "\n",
    "Variance = $\\frac{250}{5 - 1}$ = $\\frac{250}{4}$ = 62.5\n",
    "\n",
    "\n",
    "```python\n",
    "scores = [80, 85, 90, 95, 100]\n",
    "mean_score = sum(scores) / len(scores)\n",
    "variance = sum((x - mean_score) ** 2 for x in scores) / (len(scores) - 1)\n",
    "print(f\"Variance of Scores: {variance}\") # Output: Variance of Scores: 62.5\n",
    "```\n",
    "### Standard Deviation\n",
    "The standard deviation is the square root of the variance and provides a measure of the average distance of each data point from the mean.\n",
    "\n",
    "- Example:\n",
    "\n",
    "Using the same test scores:\n",
    "[80,85,90,95,100] the standard deviation is $\\sqrt{62.5}$ = 7.9\n",
    "\n",
    "```python\n",
    "import math\n",
    "\n",
    "std_dev = math.sqrt(variance)\n",
    "print(f\"Standard Deviation of Scores: {std_dev}\") # Output: Standard Deviation of Scores: 7.905694150420948\n",
    "```\n",
    "### Quantiles\n",
    "Quantiles divide a dataset into equal-sized subsets. For instance, quartiles divide the data into four equal parts.\n",
    "Quantiles are useful to identify outliers in the data\n",
    "\n",
    "- Example:\n",
    "\n",
    "Consider the following exam scores:\n",
    "[60,70,80,90,100]\n",
    "\n",
    "```python\n",
    "scores = [60, 70, 80, 90, 100]\n",
    "sorted_scores = sorted(scores)\n",
    "\n",
    "# Calculate quartiles\n",
    "q1 = sorted_scores[int(len(sorted_scores) * 0.25)]\n",
    "q2 = sorted_scores[int(len(sorted_scores) * 0.5)]  # Median\n",
    "q3 = sorted_scores[int(len(sorted_scores) * 0.75)]\n",
    "\n",
    "print(f\"1st Quartile (Q1):{q1}\") # Output: 1st Quartile (Q1):70\n",
    "print(f\"Median (Q2):{q2}\") # Output: Median (Q2):80\n",
    "print(f\"3rd Quartile (Q3):{q3}\") # Output: 3rd Quartile (Q3):90\n",
    "```"
   ],
   "metadata": {
    "id": "rQcrFW8zRBo_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# use this as a playground to experiment with statistical concepts\n",
    "\n",
    "sales = [10, 15, 20, 25, 30]\n",
    "mean_sales = sum(sales) / len(sales)\n",
    "print(f\"Mean Sales:{mean_sales}\")\n",
    "\n",
    "ages = [22, 25, 29, 50, 35]\n",
    "sorted_ages = sorted(ages)\n",
    "n = len(sorted_ages)\n",
    "median_age = sorted_ages[n // 2] if n % 2 != 0 else (sorted_ages[n // 2 - 1] + sorted_ages[n // 2]) / 2\n",
    "print(f\"Median Age: {median_age}\")\n",
    "\n",
    "from statistics import mode\n",
    "fruits = [\"Apple\", \"Banana\", \"Apple\", \"Orange\", \"Banana\", \"Banana\"]\n",
    "mode_fruit = mode(fruits)\n",
    "print(f\"Mode of Fruits: {mode_fruit}\")\n",
    "\n",
    "scores = [80, 85, 90, 95, 100]\n",
    "mean_score = sum(scores) / len(scores)\n",
    "variance = sum((x - mean_score) ** 2 for x in scores) / (len(scores) - 1)\n",
    "print(f\"Variance of Scores: {variance}\")\n",
    "\n",
    "import math\n",
    "std_dev = math.sqrt(variance)\n",
    "print(f\"Standard Deviation of Scores: {std_dev}\")\n",
    "\n",
    "scores = [60, 70, 80, 90, 100]\n",
    "sorted_scores = sorted(scores)\n",
    "\n",
    "# Calculate quartiles\n",
    "q1 = sorted_scores[int(len(sorted_scores) * 0.25)]\n",
    "q2 = sorted_scores[int(len(sorted_scores) * 0.5)]  # Median\n",
    "q3 = sorted_scores[int(len(sorted_scores) * 0.75)]\n",
    "\n",
    "print(f\"1st Quartile (Q1):{q1}\")\n",
    "print(f\"Median (Q2):{q2}\")\n",
    "print(f\"3rd Quartile (Q3):{q3}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EgSL5KivSSts",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1730191152828,
     "user_tz": -60,
     "elapsed": 432,
     "user": {
      "displayName": "Marta Lahoya",
      "userId": "07104405807276990017"
     }
    },
    "outputId": "afeb46c5-c732-4017-9a29-7eeed94e4f61"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Dataframes\n",
    "\n",
    "In Python, dataframes are widely used data structures that represent data in a tabular form‚Äîorganized in rows and columns, much like a spreadsheet or a SQL table. Dataframes make data analysis, manipulation, and visualization much easier, especially when working with large datasets.\n",
    "\n",
    "**What is a DataFrame?**\n",
    "\n",
    "A DataFrame is essentially a two-dimensional, labeled data structure with columns of potentially different types. This versatility allows users to work with heterogeneous data types within a single structure, making dataframes very powerful for data science and machine learning.\n",
    "\n",
    "**Key characteristics of a DataFrame:**\n",
    "\n",
    "- Two-dimensional: It has rows and columns, which gives it a tabular structure.\n",
    "- Labeled axes: Rows and columns can be labeled, making it easy to reference parts of the DataFrame by name rather than by position.\n",
    "- Flexible data types: Each column can hold a different data type (e.g., integers, floats, strings, dates).\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "9oO9Dp_gV0n2"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Types of DataFrames in Python\n",
    "While pandas is the most widely used library for dataframes in Python, other libraries implement dataframes with additional functionality or optimizations tailored for specific use cases.\n",
    "\n",
    "Here are some of the primary types of dataframes in Python:\n",
    "\n",
    "| **DataFrame Type** | **Best Use Case**                                                  | **Memory Usage**                           | **Description**                                                                                 |\n",
    "|--------------------|--------------------------------------------------------------------|--------------------------------------------|-------------------------------------------------------------------------------------------------|\n",
    "| **[Pandas](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html)**         | Small to medium datasets that fit in memory                       | In-memory                                  | Standard Python library for data manipulation and analysis.                                     |\n",
    "| **[Dask](https://docs.dask.org/en/stable/dataframe.html)**           | Large datasets that exceed memory limits, single-machine parallel | Out-of-core (chunk-based, parallel)        | Extension of pandas for handling large data, processing in parallel.                            |\n",
    "| **[Koalas/PySpark](https://koalas.readthedocs.io/en/latest/reference/frame.html#constructor)** | Big data processing on distributed cluster with Spark             | Distributed (cluster-based)                | Spark-compatible dataframe for big data, scalable to clusters.                                  |\n",
    "| **[Modin](https://modin.readthedocs.io/en/latest/usage_guide/index.html)**          | Pandas-compatible workflows with faster, parallel execution       | In-memory (parallel)                       | Parallelized pandas replacement for faster execution.                                           |\n",
    "| **[Polars](https://docs.pola.rs/)**         | Performance-critical applications on large datasets               | In-memory (optimized for speed)            | Rust-based dataframe optimized for speed, available in Python.                                  |\n",
    "| **[Vaex](https://vaex.io/docs/api.html)**           | Extremely large datasets without loading all data into memory     | Out-of-core (efficient memory usage)       | Handles large datasets out-of-core; efficient for exploration and stats.                        |\n"
   ],
   "metadata": {
    "id": "zkrPnky5W7CV"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "<div style=\"display: flex; justify-content: space-around;\">\n",
    "    <img src=\"https://pandas.pydata.org/static/img/pandas.svg\" alt=\"Pandas Logo\" style=\"height: 50px; max-height: 50px;\">\n",
    "    <img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTP5KVvipePSkKXNx0CLGxNfV2hnEdm13gPMA&s\" alt=\"Dask Logo\" style=\"height: 50px; max-height: 50px;\">\n",
    "    <img src=\"https://koalas.readthedocs.io/en/latest/_static/koalas-logo-docs.png\" alt=\"Koalas Logo\" style=\"height: 50px; max-height: 50px;\">\n",
    "    <img src=\"https://modin.readthedocs.io/en/latest/_images/MODIN_ver2_hrz.png\" alt=\"Modin Logo\" style=\"height: 50px; max-height: 50px;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/pola-rs/polars-static/master/logos/polars-logo-dimmed-medium.png\" alt=\"Polars Logo\" style=\"height: 50px; max-height: 50px;\">\n",
    "    <img src=\"https://vaex.io/docs/_static/logo-grey.svg\" alt=\"Vaex Logo\" style=\"height: 50px; max-height: 50px;\">\n",
    "</div>\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "zdSkQtWIZCTl"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pandas\n",
    "\n",
    "Pandas is a powerful and widely-used library in Python for data manipulation and analysis. It provides flexible and efficient data structures, primarily\n",
    "**Series** (1-dimensional) and **DataFrame** (2-dimensional), that make it easy to work with structured data. The DataFrame is especially valuable for data analytics as it allows you to store, filter, transform, and analyze datasets much like a spreadsheet or SQL table.\n",
    "\n",
    "- Data Loading and Cleaning: Pandas can load data from various sources (CSV, Excel, SQL databases, etc.) and provides tools for handling missing values, duplicates, and other inconsistencies.\n",
    "- Data Transformation: You can easily filter rows, select columns, group data, and apply complex transformations.\n",
    "- Data Aggregation and Summary Statistics: Pandas provides efficient ways to compute summary statistics (mean, sum, median, etc.) for each group or column.\n",
    "- Data Visualization Integration: Pandas integrates well with libraries like Matplotlib and Seaborn for quick visualizations.\n",
    "\n",
    "Let‚Äôs go through some examples that demonstrate common tasks in data analytics using Pandas.\n",
    "\n",
    "> For more details on especific functions [read the docs](https://pandas.pydata.org/docs/reference/index.html)\n"
   ],
   "metadata": {
    "id": "p7ojhl5Hcu80"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Installing pandas\n",
    "\n",
    "Installing pandas depends on the environment you're using.\n",
    "\n",
    "1. In Jupyter notebooks, if you need to update pandas to a specific version, you can do the following:\n",
    "\n",
    "  ```python\n",
    "  # Install or update pandas to last version\n",
    "  !pip install pandas --upgrade\n",
    "  ```\n",
    "  Run this in a Colab cell, and it will install or update pandas to the latest version.\n",
    "\n",
    "  You can install a especific version of pandas using the `==` syntax.\n",
    "  ```python\n",
    "  # Install a certain version of pandas\n",
    "  !pip install pandas==1.5.3\n",
    "```\n",
    "> **Google Colab** comes with pandas pre-installed, so you typically don't need to install it manually.\n",
    "\n",
    "2. Installing Pandas in Jupyter Notebook or Local Python Environment\n",
    "If you‚Äôre working locally in a Jupyter Notebook or any Python environment (like PyCharm, VS Code, etc.), you can use `pip to install pandas`.\n",
    "\n",
    "  Open a terminal and run:\n",
    "\n",
    "  ```bash\n",
    "  pip install pandas\n",
    "  ```\n",
    "  If you want a specific version of pandas, specify it as follows:\n",
    "\n",
    "  ```bash\n",
    "  pip install pandas==1.5.3\n",
    "  ```\n",
    "\n",
    "3. Installing Pandas in Anaconda\n",
    "If you‚Äôre using Anaconda, it‚Äôs often best to install pandas via the `conda` package manager, which manages dependencies more effectively within the Anaconda ecosystem.\n",
    "\n",
    "  Using Conda\n",
    "  Open the Anaconda Prompt and enter:\n",
    "\n",
    "  ```bash\n",
    "  conda install pandas\n",
    "  ```\n",
    "  This will install pandas and any required dependencies.\n",
    "\n",
    "  If you want a specific version of pandas, you can specify it like this:\n",
    "\n",
    "  ```bash\n",
    "  conda install pandas=1.5.3\n",
    "  ```\n",
    "  Creating a New Environment with Pandas\n",
    "  You can also create a new conda environment with pandas pre-installed:\n",
    "\n",
    "  ```bash\n",
    "  conda create -n myenv pandas\n",
    "  ```\n",
    "  Replace `myenv` with your desired environment name. To activate the environment, use:\n",
    "\n",
    "  ```bash\n",
    "  conda activate myenv\n",
    "  ```\n",
    "\n",
    "4. Verifying the Installation\n",
    "To check that pandas installed correctly, open a Python environment (Colab, Jupyter, or terminal) and run:\n",
    "\n",
    "  ```python\n",
    "  import pandas as pd\n",
    "  print(pd.__version__)  # This should display the installed pandas version\n",
    "  ```\n",
    "  This will confirm that pandas is installed and display the version."
   ],
   "metadata": {
    "id": "govF6OAjf8dD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "print(pd.__version__)  # This should display the installed pandas version"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ea9RgLgPiZyn",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1730092315147,
     "user_tz": -60,
     "elapsed": 940,
     "user": {
      "displayName": "Marta Lahoya",
      "userId": "07104405807276990017"
     }
    },
    "outputId": "18bcf83b-039d-4880-bb40-1e4eed1078d5"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Importing the Library and Loading Data\n",
    "First, let's import pandas and load a sample dataset. Here, we‚Äôll use a sample CSV file, which could represent any real-world dataset.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Load data from a CSV file\n",
    "# In real cases, you would use a file path, e.g., 'data.csv'\n",
    "df = pd.read_csv(\"sample_data.csv\")  # Replace with the path to your CSV file\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "df.head()\n",
    "```\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "2poiuogHdQOo"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "uMzgjs2WfbBm",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1730183200556,
     "user_tz": -60,
     "elapsed": 187732,
     "user": {
      "displayName": "Marta Lahoya",
      "userId": "07104405807276990017"
     }
    },
    "outputId": "ccd00608-dc4a-4ecb-8e1f-6dd4b49c43a0"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data from a CSV file\n",
    "df = pd.read_csv(\"sample_data/california_housing_test.csv\")\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "df.head()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "lCSjhwITifOh",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1730451900500,
     "user_tz": -60,
     "elapsed": 377,
     "user": {
      "displayName": "Marta Lahoya",
      "userId": "07104405807276990017"
     }
    },
    "outputId": "01ef1ca0-749f-4513-9888-52c1d1b08169"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Understading the Data\n",
    "\n",
    "1. Viewing the Structure of the DataFrame\n",
    "To get a quick overview of the DataFrame, use:\n",
    "\n",
    "```python\n",
    "df.info()\n",
    "```\n",
    "This will show you the number of entries, column names, non-null counts, and data types.\n",
    "\n",
    "```python\n",
    "df.describe()\n",
    "```\n",
    "The output is another DataFrame that includes several key statistics for each numerical column:\n",
    "\n",
    "- count: The number of non-null entries.\n",
    "- mean: The average value.\n",
    "- std: The standard deviation, which measures the amount of variation or dispersion in the data.\n",
    "- min: The minimum value.\n",
    "- 25%: The first quartile, or the 25th percentile.\n",
    "- 50%: The median, or the 50th percentile.\n",
    "- 75%: The third quartile, or the 75th percentile.\n",
    "- max: The maximum value.\n",
    "\n",
    "2. Listing Columns\n",
    "To list all the columns in the DataFrame:\n",
    "\n",
    "```python\n",
    "df.columns\n",
    "```\n",
    "This is useful if we need to iterate through the columns of the dataframe\n",
    "\n",
    "3. Finding Minimum and Maximum Values\n",
    "To find the minimum and maximum values of numerical columns:\n",
    "\n",
    "```python\n",
    "min_values = df[column].min()\n",
    "max_values = df[column].max()\n",
    "```\n",
    "> You can calculate other values like standard deviation, quantile, mean, mode and more using the same syntax.\n",
    "\n",
    "4. Counting Unique Values\n",
    "To get the number of unique values in each column:\n",
    "\n",
    "```python\n",
    "unique_counts = df[column].nunique()\n",
    "print(\"Number of unique values:\\n\", unique_counts)\n",
    "```\n",
    "You can also get the list of unique values using\n",
    "unique_values = df[column].unique()\n",
    "\n",
    "5. Counting Null Values\n",
    "To count the number of null values in each column:\n",
    "\n",
    "```python\n",
    "null_counts = df.isnull().sum()\n",
    "print(\"Number of null values:\\n\", null_counts)\n",
    "```"
   ],
   "metadata": {
    "id": "Dpan7TCmCGCW"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df.describe()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "mE_KiPvGeGg8",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1730191853074,
     "user_tz": -60,
     "elapsed": 333,
     "user": {
      "displayName": "Marta Lahoya",
      "userId": "07104405807276990017"
     }
    },
    "outputId": "6521bd7d-0f16-4042-bf4d-2289a28b3f0b"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df.info()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tEQriseBC5al",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1730191908351,
     "user_tz": -60,
     "elapsed": 1813,
     "user": {
      "displayName": "Marta Lahoya",
      "userId": "07104405807276990017"
     }
    },
    "outputId": "35facdc3-835a-4768-f61f-efded9bb5128"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df.columns\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9JWY8pShC5hZ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1730191937418,
     "user_tz": -60,
     "elapsed": 332,
     "user": {
      "displayName": "Marta Lahoya",
      "userId": "07104405807276990017"
     }
    },
    "outputId": "28f07a49-bb93-494d-ebd1-896fbf49db44"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df.dtypes"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "id": "6WZRXsTYEsWU",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1730191948648,
     "user_tz": -60,
     "elapsed": 359,
     "user": {
      "displayName": "Marta Lahoya",
      "userId": "07104405807276990017"
     }
    },
    "outputId": "ee8fcbcd-ae65-4a1c-9e74-553da5d6a45e"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Create a dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Calculate metrics for numeric columns\n",
    "numeric_df = df.select_dtypes(include='number')\n",
    "for column in numeric_df.columns:\n",
    "    results[column] = {\n",
    "        'min': numeric_df[column].min(),\n",
    "        'max': numeric_df[column].max(),\n",
    "        'mean': numeric_df[column].mean(),\n",
    "        'std': numeric_df[column].std(),\n",
    "        'count': numeric_df[column].count(),\n",
    "        'p90': numeric_df[column].quantile(0.25),\n",
    "        'p75': numeric_df[column].quantile(0.75),\n",
    "        'p50': numeric_df[column].quantile(0.5),\n",
    "        'p25': numeric_df[column].quantile(0.25),\n",
    "        'unique_count': numeric_df[column].nunique(),\n",
    "        'most_common': numeric_df[column].mode()[0] if not numeric_df[column].mode().empty else None,\n",
    "        'count_non_null': numeric_df[column].count(),\n",
    "    }\n",
    "\n",
    "# Calculate metrics for non-numeric columns\n",
    "non_numeric_df = df.select_dtypes(exclude='number')\n",
    "for column in non_numeric_df.columns:\n",
    "    results[column] = {\n",
    "        'unique_count': non_numeric_df[column].nunique(),\n",
    "        'most_common': non_numeric_df[column].mode()[0],\n",
    "        'count_non_null': non_numeric_df[column].count(),\n",
    "    }\n",
    "\n",
    "# Convert results to a DataFrame for better visualization\n",
    "# the .T attribute is used to transpose the DataFrame\n",
    "results_df = pd.DataFrame(results).T\n",
    "\n",
    "results_df\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "id": "3DCeGA1ND0po",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1730450503175,
     "user_tz": -60,
     "elapsed": 737,
     "user": {
      "displayName": "Marta Lahoya",
      "userId": "07104405807276990017"
     }
    },
    "outputId": "001b4e22-2aac-4506-d35d-641533040d72"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "source": [
    "# This will create a new interactive google sheet with the data in the dataframe passed as paramter (results_df)\n",
    "from google.colab import sheets\n",
    "sheet = sheets.InteractiveSheet(df=results_df)"
   ],
   "cell_type": "code",
   "metadata": {
    "cellView": "form",
    "id": "Tnv8EP_JK_OM",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 693
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1730450538482,
     "user_tz": -60,
     "elapsed": 29083,
     "user": {
      "displayName": "Marta Lahoya",
      "userId": "07104405807276990017"
     }
    },
    "outputId": "b5008024-0867-471f-ddfb-c01b5e459a75"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "unique_values = df['housing_median_age'].unique()\n",
    "print(unique_values)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ifs6vXi6PGm0",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1730192412024,
     "user_tz": -60,
     "elapsed": 400,
     "user": {
      "displayName": "Marta Lahoya",
      "userId": "07104405807276990017"
     }
    },
    "outputId": "128f201b-0e50-47ac-958d-9522e9436235"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# seaborn library is a powerful visualization tool built on top of matplotlib.\n",
    "# It provides a high-level interface for drawing attractive statistical graphics.\n",
    "import seaborn as sns\n",
    "\n",
    "# import the matplotlib.pyplot module and assign it to the alias 'plt'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize a new figure for the plot: 10 inches width and 6 inches height.\n",
    "plt.figure(figsize=(10, 6))\n",
    "#Histogram with Density Plot\n",
    "\"\"\"\n",
    ":\n",
    "sns.histplot(...): This function creates a histogram for the specified data.\n",
    "The bins parameter determines how many intervals will be used in the histogram.\n",
    "kde=True: enables the kernel density estimate (KDE) plot, which provides a smoothed version of the histogram.\n",
    " It shows the probability density function of the variable, helping to visualize the distribution more clearly.\n",
    "color='grey': sets the color of the histogram bars to grey.\n",
    "alpha=0.5: sets the transparency level of the histogram bars, making them semi-transparent (50% opacity), which can help in visualizing overlapping elements.\n",
    "\"\"\"\n",
    "sns.histplot(df['housing_median_age'], bins=int(df['housing_median_age'].max()) - int(df['housing_median_age'].min()), kde=True, color='grey', alpha=0.5)\n",
    "\n",
    "plt.title('Histogram with Density Plot')\n",
    "plt.xlabel('Housing median age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "ylT5gADDNmnv",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1730451915501,
     "user_tz": -60,
     "elapsed": 1317,
     "user": {
      "displayName": "Marta Lahoya",
      "userId": "07104405807276990017"
     }
    },
    "outputId": "7b4245c0-7f0a-458f-c264-bffbfdbf947b"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "unique_counts = df.nunique()\n",
    "print(\"Number of unique values:\\n\", unique_counts)\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kawsGB4MD6aZ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1730185092975,
     "user_tz": -60,
     "elapsed": 439,
     "user": {
      "displayName": "Marta Lahoya",
      "userId": "07104405807276990017"
     }
    },
    "outputId": "3b601166-5a15-45cc-ca4c-888645ea1b49"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "####üèÜ Understanding the Data Challenge\n",
    "\n",
    "**Descriptive Statistics Calculation of NYC Taxi Trips**\n",
    "\n",
    "In this challenge, you will analyze a dataset to calculate descriptive statistics for both numeric and non-numeric columns.\n",
    "\n",
    "The goal of this challenge is to create a dictionary that stores descriptive statistics for each column in the dataset.\n",
    "\n",
    "This will include metrics for numeric columns (like min, max, mean, standard deviation, percentiles, and unique counts) and key statistics for non-numeric columns (like unique counts and the most common value).\n",
    "\n",
    "Finally, you will convert this information into a DataFrame for easier visualization.\n",
    "\n",
    "The data source will be the parquet file located at https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet\n",
    "\n",
    "\n",
    "Steps to Accomplish the Challenge\n",
    "\n",
    "- Load the Dataset:\n",
    "\n",
    "  Ensure you have the dataset loaded into a DataFrame named df.\n",
    "\n",
    "- Create a Results Dictionary:\n",
    "\n",
    "  Initialize an empty dictionary to store the results of your calculations.\n",
    "\n",
    "- Calculate Metrics for **Numeric** Columns:\n",
    "\n",
    "  Use the `select_dtypes()` method to create a DataFrame containing only numeric columns.\n",
    "\n",
    "  Iterate through the numeric columns and calculate the following metrics:\n",
    "  - Minimum value\n",
    "  - Maximum value\n",
    "  - Mean\n",
    "  - Standard deviation\n",
    "  - Count of non-null values\n",
    "  - Percentiles (25th, 50th, 75th)\n",
    "  - Unique count\n",
    "  - Most common value (mode)\n",
    "  \n",
    "- Calculate Metrics for **Non-Numeric** Columns:\n",
    "\n",
    "  Create a DataFrame for non-numeric columns using select_dtypes().\n",
    "  \n",
    "  Iterate through the non-numeric columns and calculate:\n",
    "  - Unique count\n",
    "  - Most common value (mode)\n",
    "  - Count of non-null values\n",
    "\n",
    "- Convert Results to a DataFrame:\n",
    "\n",
    "  Convert the results dictionary into a DataFrame for better visualization, using the `.T` attribute to transpose it.\n",
    "\n",
    "- Display the Results:\n",
    "\n",
    "  Print the resulting DataFrame to review the summary statistics for all columns.\n"
   ],
   "metadata": {
    "id": "Gp-OFqZRIPhj"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Cleaning\n",
    "Real-world datasets often have missing or inconsistent data. Pandas makes it easy to identify and clean this data.\n",
    "\n",
    "We can apply different strategies\n",
    "\n",
    "1. Handling Missing Values\n",
    "\n",
    "  In case of a missing value we can fill it with an aggregated value like the mean of the column.\n",
    "\n",
    "  - Initial Check:\n",
    "  \n",
    "    Start with `df.isnull().sum()` to get a quick overview of null values in the dataframe\n",
    "\n",
    "  - Detailed Analysis:\n",
    "\n",
    "    If you notice a significant number of missing values in certain columns, you might want to investigate those columns further. For example, you could check the data types and consider how to handle missing values based on the type of data (`mean` for numeric, `mode` for categorical, etc.).\n",
    "\n",
    "  ```python  \n",
    "  # Check for missing values\n",
    "  print(f\"Number of null values:\\n{df.isnull().sum()}\")\n",
    "\n",
    "  # Fill missing values\n",
    "  for column in df.columns:\n",
    "      if df[column].dtype in ['float64', 'int64']:  # Check if the column is numeric\n",
    "          df[column].fillna(df[column].mean(), inplace=True)  # Fill with mean\n",
    "      elif df[column].dtype == 'object':  # Check if the column is categorical\n",
    "          df[column].fillna(df[column].mode()[0], inplace=True)  # Fill with mode\n",
    "\n",
    "  # Verify that there are no more missing values\n",
    "  print(f\"Number of null values after filling:\\n{df.isnull().sum()}\")\n",
    "  ```\n",
    "\n",
    "2. Dropping Duplicates\n",
    "\n",
    "  In case of missing values we can also delete all the row.\n",
    "  ```python\n",
    "  # Drop duplicate rows, if any\n",
    "  df.drop_duplicates(inplace=True)\n",
    "  ```\n"
   ],
   "metadata": {
    "id": "XlbVyr5QeHhk"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "####üèÜ Data Cleaning Challenge\n",
    "\n",
    "**Data Cleaning of Taxi Trip Data**\n",
    "\n",
    "In this challenge, you will work with a dataset of yellow taxi trip data to perform data cleaning tasks. Your objective is to handle missing values in the dataset appropriately, ensuring that it is ready for analysis.\n",
    "\n",
    "The data source will be the parquet file located at https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet\n",
    "\n",
    "Steps to Accomplish the Challenge\n",
    "\n",
    "- Load the Dataset:\n",
    "\n",
    "  Use the pandas library to read the parquet file from the provided URL and store it in a DataFrame named df_taxi.\n",
    "  \n",
    "- Check for Missing Values:\n",
    "\n",
    "  Print the number of null values in each column of the DataFrame to understand the extent of missing data.\n",
    "\n",
    "- Fill Missing Values:\n",
    "\n",
    "  Iterate through each column in the DataFrame:\n",
    "  - For numeric columns (of type `float64` or `int64`), fill missing values with the mean of that column.\n",
    "  - For categorical columns (of type `object`), fill missing values with the mode (most frequent value) of that column.\n",
    "\n",
    "- Verify Completion:\n",
    "\n",
    "  Print the number of null values in each column again to confirm that all missing values have been successfully filled.\n",
    "\n"
   ],
   "metadata": {
    "id": "eWKbXDWu_H32"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "QMKHtewMrgHY",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1730452600842,
     "user_tz": -60,
     "elapsed": 363,
     "user": {
      "displayName": "Marta Lahoya",
      "userId": "07104405807276990017"
     }
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Filtering and Selecting Data\n",
    "Pandas makes it easy to filter data based on certain conditions and select specific columns.\n",
    "\n",
    "1. Selecting Columns\n",
    "  ```python\n",
    "  # Select specific columns\n",
    "  subset = df[['Column1', 'Column2']]\n",
    "  ```\n",
    "\n",
    "2. Filtering Rows Based on Conditions\n",
    "  ```python\n",
    "  # Filter rows where 'Column1' > 50\n",
    "  filtered_data = df[df['Column1'] > 50]\n",
    "  ```\n",
    "\n",
    "  To apply multiple conditions when filtering a DataFrame in pandas, you should use the bitwise operators `&` (for AND) or `|` (for OR) instead of the `and` keyword. Additionally, each condition needs to be enclosed in parentheses\n",
    "\n",
    "  ```python\n",
    "  # Filter rows where 'Column1' > 50 and at the same time 'Column2' > 0\n",
    "  filtered_data = df[(df['Column1'] > 50) & (df['Column2']> 0)]\n",
    "  ```\n"
   ],
   "metadata": {
    "id": "8-O8Rmzhd26E"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "####üèÜ Filtering and Selecting Data challenge\n",
    "\n",
    "**Filtering Taxi Trip Data**\n",
    "\n",
    "In this challenge, you will work with a dataset of yellow taxi trip data to filter out trips based on specific criteria related to passenger count and trip distance. Your objective is to create a new DataFrame that includes only valid taxi trips for further analysis.\n",
    "\n",
    "The data source will be the parquet file located at https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet\n",
    "\n",
    "Conditions:\n",
    "- the number of passengers is between 1 and 5 (inclusive)\n",
    "- the trip distance is greater than 0.\n",
    "\n",
    "The final result dataframe should include only the following columns:\n",
    "```\n",
    "'passenger_count', 'trip_distance', 'fare_amount', 'tip_amount', 'total_amount', 'VendorID','tpep_pickup_datetime', 'tpep_dropoff_datetime'\n",
    "```\n",
    "\n",
    "This will help ensure that the data you analyze is relevant and meets specific criteria.\n",
    "\n",
    "Steps to Accomplish the Challenge\n",
    "- Load the Dataset:\n",
    "\n",
    "  Ensure you have the yellow taxi trip dataset loaded into a DataFrame named df_taxi.\n",
    "\n",
    "- Understand the Data:\n",
    "\n",
    "  Familiarize yourself with the DataFrame's structure, including the columns and their data types, focusing on passenger_count and trip_distance.\n",
    "\n",
    "- Filter the Data:\n",
    "\n",
    "  Use boolean indexing to create a new DataFrame, filtered_data, that meets the following conditions:\n",
    "  The passenger_count must be greater than 0 and less than or equal to 5.\n",
    "  The trip_distance must be greater than 0.\n",
    "\n",
    "- Filter Columns:\n",
    "\n",
    "  Include only the columns that are relevant for the analysis.\n",
    "\n",
    "- Verify the Filtered Data:\n",
    "\n",
    "  Check the shape or the first few rows of the filtered_data DataFrame to ensure the filtering was applied correctly and that it contains only the desired trips.\n",
    "\n",
    "\n",
    "*passenger_count*>0 and passenger_count<=5"
   ],
   "metadata": {
    "id": "OVX8GFF8D0L1"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Grouping and Aggregating Data\n",
    "Grouping data is essential in data analytics to calculate summary statistics for different categories.\n",
    "\n",
    "We use `groupby` to group the DataFrame by the indicated column. Each unique value of it will create a separate group. For example, if there are groups for 1, 2, 3, etc., each of these groups will consist of rows.\n",
    "\n",
    "There are multiple aggregate functions that can be used:\n",
    "\n",
    "- `sum()`:Calculates the sum of the values for each group.\n",
    "```python\n",
    "grouped_data = df.groupby('CategoryColumn').sum().reset_index()\n",
    "```\n",
    "- `count()` :Counts the number of non-null values for each group.\n",
    "```python\n",
    "grouped_data = df.groupby('CategoryColumn').count().reset_index()\n",
    "```\n",
    "- `min()`:Finds the minimum value for each group.\n",
    "```python\n",
    "grouped_data = df.groupby('CategoryColumn').min().reset_index()\n",
    "```\n",
    "- `max()`: Finds the maximum value for each group.\n",
    "```python\n",
    "grouped_data = df.groupby('CategoryColumn').max().reset_index()\n",
    "```\n",
    "- `std()`: Calculates the standard deviation of the values for each group.\n",
    "```python\n",
    "grouped_data = df.groupby('CategoryColumn').std().reset_index()\n",
    "```\n",
    "- `median()`: Calculates the median value for each group.\n",
    "```python\n",
    "grouped_data = df.groupby('CategoryColumn').median().reset_index()\n",
    "```\n",
    "- `first()`: Returns the first value in each group.\n",
    "```python\n",
    "import pandas as pd\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'Category': ['A', 'A', 'B', 'B', 'C', 'C'],\n",
    "    'Value': [10, 15, 10, 5, 20, 25],\n",
    "    'Date': ['2024-01-01', '2024-01-02', '2024-02-01', '2024-02-03', '2024-03-02', '2024-03-01']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "# Convert 'Date' to datetime\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "# Sort the DataFrame by 'Category' and 'Date'\n",
    "df_sorted = df.sort_values(by=['Category', 'Date'])\n",
    "# Group by 'Category' and get the first entry for each group\n",
    "first_entries = df_sorted.groupby('Category').first().reset_index()\n",
    "print(first_entries)\n",
    "```\n",
    "- `agg()`:   The `agg()` function is used to specify the aggregation operations that will be applied to each group created by the `groupby()`. This allows you to calculate multiple statistics in a single step.\n",
    "\n",
    "  Inside `agg()`, each aggregation is defined in the format `new_column_name=('original_column_name', 'function')`.\n",
    "  This creates a new column named new_column_name in the resulting DataFrame.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> If you don‚Äôt add `reset_index()` after using `groupby()` and `agg()`, the result will be a DataFrame with a hierarchical index (multi-index) that consists of the grouping columns (in this case, passenger_count). This means that the grouping column(s) will become the index of the resulting DataFrame rather than regular columns.\n",
    "\n",
    "**Implications of Not Using reset_index()**\n",
    "\n",
    "- Accessing Data: You will have to use the .loc[] or .iloc[] methods to access the data, which can be less intuitive than working with a standard DataFrame.\n",
    "\n",
    "- DataFrame Appearance: The DataFrame will look different because the columns used to group by will be part of the index rather than a column, which may make it harder to interpret at a glance.\n",
    "\n",
    "- Subsequent Operations: If you want to perform further operations that expect the grouping variable to be a column (like merging, filtering, or plotting), you may need to reset the index later on, which can add extra steps to your workflow."
   ],
   "metadata": {
    "id": "SgpaCtvMd5Il"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Use this as a playground to explore with grouping and aggregations"
   ],
   "metadata": {
    "id": "u_toQZuNTFMI",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1730458289559,
     "user_tz": -60,
     "elapsed": 351,
     "user": {
      "displayName": "Marta Lahoya",
      "userId": "07104405807276990017"
     }
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "####üèÜ Grouping and Aggregating Data Challenge\n",
    "\n",
    "**Does the number of passengers affect the tip amount for the taxi trip?**\n",
    "\n",
    "You are tasked with analyzing taxi trip data to understand how passenger count affects tipping behavior and trip characteristics.\n",
    "\n",
    "Using the filtered DataFrame from the previous challenge, calculate the average tip amount, average trip distance, and the total number of trips for different passenger counts.\n",
    "\n",
    "- The average tip amount for each group of passenger counts.\n",
    "- The average trip distance for each group.\n",
    "- The total number of trips for each group.\n",
    "\n",
    "By completing this challenge, you will gain insights into the relationship between passenger count and trip metrics, which can be valuable for understanding customer behavior and improving service.\n",
    "\n",
    "Steps to Accomplish the Challenge\n",
    "- Import Required Libraries:\n",
    "\n",
    "  Ensure you have the necessary libraries imported, particularly pandas.\n",
    "\n",
    "- Load the Data:\n",
    "\n",
    "  Load the taxi trip data into a DataFrame. If using a filtered DataFrame (e.g., filtered_data), make sure it has already been created based on relevant criteria (such as valid passenger counts).\n",
    "\n",
    "- Group the Data:\n",
    "\n",
    "  Use the groupby() method on the passenger_count column to group the data accordingly.\n",
    "\n",
    "- Aggregate the Statistics:\n",
    "\n",
    "  Use the agg() function to compute:\n",
    "  - The average of the tip_amount column (average tip).\n",
    "  - The average of the trip_distance column (average trip distance).\n",
    "  - The count of non-null values (number of trips).\n",
    "\n",
    "  Ensure to name the resulting columns appropriately.\n",
    "\n",
    "- Reset the Index:\n",
    "\n",
    "  Call reset_index() to convert the grouped object back into a standard DataFrame format, making it easier to read and manipulate.\n",
    "\n",
    "- Display the Results:\n",
    "\n",
    "  Print or display the resulting DataFrame average_stats to view the aggregated statistics."
   ],
   "metadata": {
    "id": "mR6gIbnpUeMs"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Visualization with Pandas\n",
    "You can create quick plots directly from pandas, which integrates well with Matplotlib for more complex visualizations.\n",
    "\n",
    "Matplotlib\n",
    "\n",
    "- Line Chart: Used to display data points connected by straight lines, ideal for showing trends over time. `plt.plot(x, y)`\n",
    "- Bar Chart: Displays data with rectangular bars representing the values, useful for comparing different categories. `plt.bar(x, height)`\n",
    "- Horizontal Bar Chart: Similar to a bar chart, but bars are displayed horizontally. `plt.barh(y, width)`\n",
    "- Histogram: Used to represent the distribution of numerical data by dividing the data into bins. `plt.hist(data, bins=10)`\n",
    "- Scatter Plot: Displays individual data points using Cartesian coordinates, useful for showing relationships between two variables.\n",
    "`plt.scatter(x, y)`\n",
    "- Pie Chart: Represents proportions of a whole as slices of a circle, suitable for showing percentage distributions.\n",
    "`plt.pie(sizes, labels=labels)`\n",
    "- Box Plot: Displays the distribution of data based on five summary statistics: minimum, first quartile, median, third quartile, and maximum. `plt.boxplot(data)`\n",
    "- Heatmap: Represents data values in a matrix format using color gradients, useful for visualizing correlations or patterns. `plt.imshow(data, cmap='hot', interpolation='nearest')`\n",
    "- Area Chart: Similar to line charts, but the area below the line is filled in, highlighting the volume of data. `plt.fill_between(x, y)`\n",
    "- Violin Plot: Combines box plot and kernel density plot to show data distribution and density, useful for comparing distributions between multiple groups. `plt.violinplot(data)`\n",
    "\n",
    "\n",
    "[Documentation](https://matplotlib.org/)\n",
    "\n",
    "![Matplotlib cheat sheet](https://matplotlib.org/cheatsheets/_images/cheatsheets-1.png)(https://matplotlib.org/cheatsheets/)\n",
    "\n",
    "![Matplotlib cheat sheet](https://matplotlib.org/cheatsheets/_images/cheatsheets-2.png)(https://matplotlib.org/cheatsheets/)\n"
   ],
   "metadata": {
    "id": "BJee58SddlZd"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "####  Simple Line Plot\n",
    "\n",
    "A simple line chart is best used in the following scenarios:\n",
    "\n",
    "- **Time Series Data**: When you want to visualize trends over time, such as daily, monthly, or yearly data. Line charts effectively show how values change over intervals.\n",
    "\n",
    "- **Continuous Data**: Ideal for displaying continuous data where you expect smooth transitions between points, such as temperature changes, stock prices, or sales figures.\n",
    "\n",
    "- **Comparison of Multiple Series**: When you need to compare multiple related datasets over the same time period. Multiple lines can be plotted on the same chart to show how different groups compare over time.\n",
    "\n",
    "- **Highlighting Trends**: Use line charts to identify upward or downward trends and cycles in the data, helping to visualize long-term trends effectively.\n",
    "\n",
    "- **Simple Relationships**: When you want to illustrate the relationship between two continuous variables (e.g., time and temperature) without the need for more complex visualizations.\n",
    "\n",
    "Overall, a simple line chart is a powerful and straightforward tool for presenting data in a way that highlights changes, trends, and comparisons clearly.\n",
    "\n",
    "\n",
    "```python\n",
    "# Plot a line chart for a time series\n",
    "df['DateColumn'] = pd.to_datetime(df['DateColumn'])\n",
    "df.set_index('DateColumn')['NumericalColumn'].plot(title='Time Series Plot')\n",
    "```\n"
   ],
   "metadata": {
    "id": "KH01xGxpYW5p"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Generate sample data for a month\n",
    "date_range = pd.date_range(start='2024-01-01', end='2024-01-31')\n",
    "np.random.seed(0)  # For reproducibility\n",
    "numerical_values = np.random.randint(0, 100, size=len(date_range))\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'DateColumn': date_range,\n",
    "    'NumericalColumn': numerical_values\n",
    "})\n",
    "\n",
    "# Convert 'DateColumn' to datetime\n",
    "df['DateColumn'] = pd.to_datetime(df['DateColumn'])\n",
    "\n",
    "# Plot a line chart for the time series\n",
    "df.set_index('DateColumn')['NumericalColumn'].plot(title='Time Series Plot for January 2024')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "\n",
    "# Adjust the y-axis to start at 0\n",
    "plt.ylim(0, df['NumericalColumn'].max()+10)  # Set the y-axis limits\n",
    "\n",
    "plt.grid()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "id": "NARlGpccW25i",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1730458641791,
     "user_tz": -60,
     "elapsed": 855,
     "user": {
      "displayName": "Marta Lahoya",
      "userId": "07104405807276990017"
     }
    },
    "outputId": "33abe9af-39f8-4ac2-ba33-19a8ed8b8765"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### üèÜ Line Chart Challenge\n",
    "\n",
    "**Analyzing Daily Taxi Trip Data**\n",
    "\n",
    "The objective of this challenge is to analyze and visualize daily taxi trip data for January 2024. You will calculate the number of trips taken each day and create a time series plot to display this information clearly.\n",
    "\n",
    "Steps to Accomplish the Challenge:\n",
    "- Import Necessary Libraries:\n",
    "- Load Your Data: Load the taxi trip data into a DataFrame.You can use the data generated in the previous challenge.\n",
    "- Convert Timestamp to Datetime:\n",
    "Convert the tpep_pickup_datetime column to a datetime format for easier manipulation.\n",
    "- Extract the Date: Create a new column called date_pickup that contains only the date part of the tpep_pickup_datetime.\n",
    "- Group Data by Date: Group the data by the date_pickup column and calculate the number of trips (using the count of tip_amount) for each day.\n",
    "- Create a Time Series Plot: Plot the number of trips against the date using a line chart. Set the date_pickup column as the index.\n",
    "- Customize the Plot: Add labels for the x-axis and y-axis.\n",
    "Set the x-axis limits to display only the data for January 2024.\n",
    "Rotate x-axis labels to 90 degrees for better readability.\n",
    "Enable grid lines for better readability and display the plot.\n"
   ],
   "metadata": {
    "id": "kFN1gAUvCrhP"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Bar Plot for Categorical Data\n",
    "\n",
    "Bar plot charts are particularly useful in the following scenarios:\n",
    "\n",
    "- **Categorical Data Comparison**: When you need to compare quantities across different categories. Bar plots clearly display the differences in counts or measurements among distinct groups.\n",
    "\n",
    "- **Discrete Variables**: Ideal for visualizing discrete data where categories are non-numerical, such as survey responses (e.g., preferences, types of products, etc.).\n",
    "\n",
    "- **Ranking**: Bar charts can effectively show the ranking of categories based on their values. For example, you might use a bar plot to display sales figures for different products, making it easy to see which product performed best.\n",
    "\n",
    "- **Frequency Distribution**: When you want to show the frequency distribution of a categorical variable, such as the number of occurrences of different species in a dataset.\n",
    "\n",
    "- **Multiple Series Comparison**: Bar plots can display multiple datasets side-by-side (grouped bar plots) or stacked, allowing for easy comparison of different categories across multiple groups (e.g., sales by region and by product).\n",
    "\n",
    "- **Data Presentation**: When you need a clear and straightforward way to present data in reports or presentations, bar plots are easily understood and visually impactful.\n",
    "\n",
    "- **Visualization of Changes Over Time** (for Categorical Data): While line charts are generally preferred for continuous data over time, bar plots can also effectively display changes in categorical data over discrete time intervals (e.g., monthly sales by category).\n",
    "\n",
    "Overall, bar plots are a versatile tool for visualizing categorical data and comparing quantities, making them a staple in data analysis and presentation.\n",
    "\n",
    "```python\n",
    "# Plot a bar chart for the counts of each category\n",
    "df['CategoryColumn'].value_counts().plot(kind='bar', title='Category Counts')\n",
    "```\n"
   ],
   "metadata": {
    "id": "B7vlAt4sYbVd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data for categorical counts\n",
    "data = {\n",
    "    'CategoryColumn': ['A', 'B', 'A', 'C', 'B', 'A', 'C', 'B', 'A', 'D', 'D', 'C']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Count occurrences of each category\n",
    "category_counts = df['CategoryColumn'].value_counts()\n",
    "\n",
    "# Plot a bar chart for the counts of each category\n",
    "plt.figure(figsize=(10, 6))  # Set the figure size\n",
    "category_counts.plot(kind='bar', color='skyblue', edgecolor='black', title='Category Counts')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)  # Rotate x-axis labels for better readability\n",
    "\n",
    "# Adding a grid for easier reading\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Display the value on top of each bar\n",
    "for index, value in enumerate(category_counts):\n",
    "    plt.text(index, value, str(value), ha='center', va='bottom')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()  # Adjust layout for better spacing\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "pREjhWoaZDGl",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1730458960650,
     "user_tz": -60,
     "elapsed": 779,
     "user": {
      "displayName": "Marta Lahoya",
      "userId": "07104405807276990017"
     }
    },
    "outputId": "07559621-7cf8-481d-a41d-f5304df380ce"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "####üèÜ Bar Chart Challenge\n",
    "\n",
    "**Analyzing Taxi Trip Volume by Vendor**\n",
    "\n",
    "The objective of this challenge is to analyze and visualize the volume of taxi trips per vendor using the provided dataset from the Filtering and Selecting Data Challenge. You will count the number of trips associated with each vendor and create a bar chart to display this information clearly.\n",
    "\n",
    "Steps to Accomplish the Challenge:\n",
    "- Import Necessary Libraries\n",
    "- Load Your Data\n",
    "- Count Trip Occurrences per Vendor\n",
    "- Create a Bar Chart\n",
    "- Add Labels and Titles: Label the x-axis as \"Vendor ID\" and the y-axis as \"Trips\". Rotate x-axis labels if necessary for better readability.\n",
    "- Enhance Readability with Grid Lines\n",
    "- Display Values on Top of Bars\n",
    "- Show the Plot\n",
    "\n",
    "By following these steps, you will generate a bar chart that visually represents the volume of taxi trips for each vendor. This visualization will help identify trends in trip distribution among different vendors and provide insights into their relative performance."
   ],
   "metadata": {
    "id": "9mEUyY2iFoca"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Scatter Plot for Numerical Relationships\n",
    "\n",
    "Scatter plots are useful in the following scenarios:\n",
    "\n",
    "- **Relationship Between Two Variables**: When you want to explore the relationship or correlation between two numerical variables. Scatter plots can reveal whether a positive, negative, or no correlation exists.\n",
    "\n",
    "- **Identifying Trends**: They help visualize trends in data, showing how one variable changes in relation to another. For example, you might use a scatter plot to analyze how temperature affects ice cream sales.\n",
    "\n",
    "- **Outlier Detection**: Scatter plots are effective for identifying outliers or anomalies in the data. Points that fall far away from the general cluster of data can indicate unusual behavior or errors in data collection.\n",
    "\n",
    "- **Distribution Visualization**: When you need to see the distribution of data points across two dimensions. This can help in understanding the density and spread of data.\n",
    "\n",
    "- **Multivariable Analysis**: If you want to explore potential interactions between two variables while considering other variables, scatter plots can be enhanced with color or size encoding to represent additional data dimensions.\n",
    "\n",
    "- **Comparative Analysis**: They can be used to compare different groups or categories within the data. By using different colors or markers for different categories, you can see how they relate to each other.\n",
    "\n",
    "- **Regression Analysis**: Scatter plots are often the first step in regression analysis, allowing you to visualize the data before fitting a regression model to understand the relationship better.\n",
    "\n",
    "In summary, scatter plots are a powerful tool for visualizing relationships and distributions in numerical data, making them invaluable for exploratory data analysis and hypothesis testing.\n",
    "\n",
    "```python\n",
    "# Plot a scatter plot to see relationships between two numerical columns\n",
    "df.plot(kind='scatter', x='NumericalColumn1', y='NumericalColumn2', title='Scatter Plot')\n",
    "```"
   ],
   "metadata": {
    "id": "PWVAghsBYeUk"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Sample data for two numerical columns\n",
    "np.random.seed(0)  # For reproducibility\n",
    "data = {\n",
    "    'NumericalColumn1': np.random.rand(100) * 100,  # Random values between 0 and 100\n",
    "    'NumericalColumn2': np.random.rand(100) * 100  # Random values between 0 and 100\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create a scatter plot to see relationships between two numerical columns\n",
    "plt.figure(figsize=(10, 6))  # Set the figure size\n",
    "plt.scatter(df['NumericalColumn1'], df['NumericalColumn2'],\n",
    "            color='blue', alpha=0.6, edgecolor='black')\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Scatter Plot of NumericalColumn1 vs NumericalColumn2')\n",
    "plt.xlabel('NumericalColumn1')\n",
    "plt.ylabel('NumericalColumn2')\n",
    "\n",
    "# Adding a grid for better readability\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Optionally, you can add a regression line (if applicable)\n",
    "# Here, we calculate a simple linear regression line\n",
    "m, b = np.polyfit(df['NumericalColumn1'], df['NumericalColumn2'], 1)\n",
    "plt.plot(df['NumericalColumn1'], m * df['NumericalColumn1'] + b, color='red', linewidth=2)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()  # Adjust layout for better spacing\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "64ClJ7T5Z5mI",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1730459184346,
     "user_tz": -60,
     "elapsed": 663,
     "user": {
      "displayName": "Marta Lahoya",
      "userId": "07104405807276990017"
     }
    },
    "outputId": "576f1841-54c0-47b6-8db5-45735326af31"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### üèÜ Scatterplot Chart Challenge\n",
    "\n",
    "**Analyzing Trip Data Relationships**\n",
    "\n",
    "The goal of this challenge is to analyze the relationship between the total amount charged for taxi trips and the distance traveled. This involves cleaning the dataset by removing outliers and correcting data issues, followed by visualizing the results with a scatter plot and a regression line.\n",
    "\n",
    "High-Level Steps:\n",
    "\n",
    "- Data Cleaning:\n",
    "\n",
    "  - Identify and filter out outlier trips with excessive trip distances (greater than 200 kilometers).\n",
    "  - Convert any negative values in the total_amount column to their absolute values to ensure all amounts are non-negative.\n",
    "\n",
    "- Data Visualization:\n",
    "\n",
    "  - Create a scatter plot to visualize the relationship between total_amount and trip_distance.\n",
    "  - Set appropriate titles and labels for clarity, including a grid for better readability.\n",
    "\n",
    "- Regression Analysis:\n",
    "\n",
    "  - Calculate the linear regression line to understand the correlation between the two variables. Use `numpy.polyfit()` to obtain the slope and intercept.\n",
    "  - Generate a range of x-values for the regression line based on the cleaned total_amount data.\n",
    "  - Calculate the corresponding y-values using the linear regression equation.\n",
    "\n",
    "- Final Visualization:\n",
    "\n",
    "  Plot the regression line over the scatter plot to illustrate the relationship between the total amount charged and trip distance.\n",
    "  Display the final plot with proper layout adjustments for optimal viewing.\n",
    "\n",
    "This challenge will help you practice data cleaning, visualization techniques, and regression analysis using Python and its libraries."
   ],
   "metadata": {
    "id": "661xm3SdK-yU"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Advanced Data Analytics: Calculating Correlations\n",
    "Correlation analysis helps identify relationships between numerical columns, often a key part of data analytics.\n",
    "\n",
    "```python\n",
    "\n",
    "# Calculate correlations between numerical columns\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Display the correlation matrix\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Plot a heatmap (requires seaborn library)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n",
    "```\n"
   ],
   "metadata": {
    "id": "xP8AhPcodTKE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data creation\n",
    "np.random.seed(0)  # For reproducibility\n",
    "\n",
    "# Creating a DataFrame with realistic numerical data\n",
    "data = {\n",
    "    'Sales': np.random.normal(200, 50, 1000),  # Normal distribution of sales\n",
    "    'Advertising': np.random.normal(50, 10, 1000),  # Advertising expenses\n",
    "    'Profit': np.random.normal(30, 5, 1000),  # Profit values\n",
    "    'Customer_Rating': np.random.uniform(1, 5, 1000),  # Customer ratings from 1 to 5\n",
    "    'Product_Cost': np.random.normal(20, 5, 1000)  # Cost of the product\n",
    "}\n",
    "\n",
    "# Adjusting profit to be related to sales and advertising\n",
    "data['Profit'] = data['Sales'] * 0.15 - data['Product_Cost'] + np.random.normal(0, 5, 1000)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate correlations between numerical columns\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Display the correlation matrix\n",
    "print(\"Correlation Matrix:\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Plot a heatmap\n",
    "plt.figure(figsize=(10, 8))  # Set the figure size\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", square=True, linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 942
    },
    "id": "AWHVOYcrae7B",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1730459338598,
     "user_tz": -60,
     "elapsed": 1003,
     "user": {
      "displayName": "Marta Lahoya",
      "userId": "07104405807276990017"
     }
    },
    "outputId": "ec360480-fed7-4ac8-e5cb-9a625736545a"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Applying Functions to Columns in DataFrames\n",
    "In data analytics, applying functions to columns of a DataFrame is a powerful technique for transforming and manipulating data. This can be useful for a variety of scenarios, such as data cleaning, feature engineering, and deriving new insights from existing data.\n",
    "\n",
    "Here, we will explore several examples and use cases to illustrate how to apply functions to columns in a pandas DataFrame.\n"
   ],
   "metadata": {
    "id": "uyGYfO7Qdb8f"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Squaring Values in a Numerical Column\n",
    "\n",
    "Use Case: This transformation could be used in a scenario where you want to create polynomial features for a regression model.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "Q9FlPLa1Lf6r"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = pd.DataFrame({\n",
    "    'NumericalColumn': [1, 2, 3, 4, 5]\n",
    "})\n",
    "\n",
    "# Apply a custom function to square the values in 'NumericalColumn'\n",
    "data['SquaredValues'] = data['NumericalColumn'].apply(lambda x: x ** 2)\n",
    "\n",
    "print(data)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-zxZjHLmMiZj",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1730472458685,
     "user_tz": -60,
     "elapsed": 184,
     "user": {
      "displayName": "Marta Lahoya",
      "userId": "07104405807276990017"
     }
    },
    "outputId": "6718aa35-318a-44b7-9866-7c2ab8ab80ee"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Converting Units\n",
    "\n",
    "Use Case: This is useful in scenarios where temperature data needs to be standardized to a specific unit for analysis or visualization.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "SVlTPc6BMe56"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Sample data with temperature in Celsius\n",
    "temperature_data = pd.DataFrame({\n",
    "    'Celsius': [0, 20, 100, -10]\n",
    "})\n",
    "\n",
    "# Function to convert Celsius to Fahrenheit\n",
    "def celsius_to_fahrenheit(celsius):\n",
    "    return (celsius * 9/5) + 32\n",
    "\n",
    "# Apply the conversion function\n",
    "temperature_data['Fahrenheit'] = temperature_data['Celsius'].apply(celsius_to_fahrenheit)\n",
    "\n",
    "print(temperature_data)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xRICfzHzMZ6d",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1730472422469,
     "user_tz": -60,
     "elapsed": 182,
     "user": {
      "displayName": "Marta Lahoya",
      "userId": "07104405807276990017"
     }
    },
    "outputId": "573da9dc-6b94-4eca-9c96-687f97fac146"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Text Processing\n",
    "\n",
    "Use Case: This can be particularly useful in data cleaning, where inconsistent capitalization in text data needs to be standardized before further analysis or reporting.\n"
   ],
   "metadata": {
    "id": "h667M5q9MWc4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "# Sample data with names\n",
    "name_data = pd.DataFrame({\n",
    "    'Names': ['alice', 'BOB', 'Charlie', 'dave']\n",
    "})\n",
    "\n",
    "# Apply a function to capitalize the first letter of each name\n",
    "name_data['CapitalizedNames'] = name_data['Names'].apply(lambda x: x.capitalize())\n",
    "\n",
    "print(name_data)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "etGazRkDMMNB",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1730472378425,
     "user_tz": -60,
     "elapsed": 196,
     "user": {
      "displayName": "Marta Lahoya",
      "userId": "07104405807276990017"
     }
    },
    "outputId": "b6b0d097-fc73-4bc5-d68e-21593d384b4b"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Conditional Transformations\n",
    "\n",
    "Use Case: This transformation can help categorize numerical values into discrete categories, which is useful for summarizing performance metrics.\n"
   ],
   "metadata": {
    "id": "qX3H2270MJFi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "# Sample data with scores\n",
    "scores_data = pd.DataFrame({\n",
    "    'Scores': [85, 92, 78, 90, 60]\n",
    "})\n",
    "\n",
    "# Apply a function to categorize scores\n",
    "scores_data['Grade'] = scores_data['Scores'].apply(lambda x: 'A' if x >= 90 else ('B' if x >= 80 else 'C'))\n",
    "\n",
    "print(scores_data)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ylwmu2HMBY7",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1730472330874,
     "user_tz": -60,
     "elapsed": 292,
     "user": {
      "displayName": "Marta Lahoya",
      "userId": "07104405807276990017"
     }
    },
    "outputId": "1ae3238a-6c01-4236-942a-b66994af0779"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Date Manipulation\n",
    "\n",
    "Use Case: Extracting specific date components is crucial for time series analysis and allows for grouping or filtering by time periods.\n"
   ],
   "metadata": {
    "id": "XVNDTzTKLi-e"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HAzi3SNWVYrh",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1730472247267,
     "user_tz": -60,
     "elapsed": 1164,
     "user": {
      "displayName": "Marta Lahoya",
      "userId": "07104405807276990017"
     }
    },
    "outputId": "e603458f-29d0-4996-934e-0da1643b9dd1"
   },
   "source": [
    "\n",
    "# Sample data with dates\n",
    "date_data = pd.DataFrame({\n",
    "    'Date': ['2023-01-01', '2023-06-15', '2023-09-20']\n",
    "})\n",
    "\n",
    "# Convert to datetime format and extract the month\n",
    "date_data['Date'] = pd.to_datetime(date_data['Date'])\n",
    "date_data['Month'] = date_data['Date'].apply(lambda x: x.month)\n",
    "\n",
    "print(date_data)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "92vu7kDgV0TU"
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
